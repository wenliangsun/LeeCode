# 机器学习问题总结

## 统计学习方法方法特点概括总结

| 方法                 | 适用问题         | 模型特点                                           | 模型类型 | 学习策略                           | 学习的损失函数       | 学习算法                                   |
| -------------------- | ---------------- | -------------------------------------------------- | -------- | ---------------------------------- | -------------------- | ------------------------------------------ |
| 感知机               | 二分类           | 分离超平面                                         | 判别类型 | 极小化误分点到超平面的距离         | 误分点到超平面的距离 | 随机梯度下降                               |
| K 近邻               | 多类分类，回归   | 特征空间，样本点                                   | 判别模型 | -                                  | -                    | -                                          |
| 朴素贝叶斯法         | 多类分类         | 特征与类别的联合概率分布，属性条件独立性假设       | 生成模型 | 极大似然估计，极大后验概率估计     | 对数似然损失         | 概率计算公式，EM算法                       |
| 决策树               | 多类分类，回归   | 分类树，回归树                                     | 判别模型 | 正则化的极大似然估计               | 对数似然损失         | 特征选择，生成，剪枝                       |
| 逻辑回归与最大熵模型 | 二分类，多类分类 | 特征条件下类别的条件概率分布，对数线性模型         | 判别模型 | 极大似然估计，正则化的极大似然估计 | 对数损失函数         | 改进的迭代尺度算法，梯度下降算法，拟牛顿法 |
| 支持向量机           | 二分类           | 分离超平面，核技巧                                 | 判别模型 | 极小化正则化合页损失，软间隔最大化 | 合页损失             | 序列最小最优化算法SMO                      |
| 提升方法             | 二分类           | 弱分类器的线性组合                                 | 判别模型 | 极小化加法模型的指数损失           | 指数损失             | 前向分步加法算法                           |
| EM算法               | 概率模型参数估计 | 含隐变量概率模型                                   | -        | 极大似然估计，极大后验概率估计     | 对数似然损失         | 迭代算法                                   |
| 隐马尔科夫模型       | 标注             | 观测序列与状态序列的联合概率分布模型               | 生成模型 | 极大似然估计，极大后验概率估计     | 对数似然损失         | 概率计算公式，EM算法                       |
| 条件随机场           | 标注             | 状态序列条件下观测序列的条件概率分布，对数线性模型 | 判别模型 | 极大似然估计，正则化极大似然估计   | 对数似然损失函数     | 改进的迭代尺度算法，梯度下降算法，拟牛顿法 |

## 1. 逻辑回归为什么不用均方误差作为损失函数，而是通过最大似然估计来求参数的？

对于逻辑回归，这里所说的对数损失和极大似然是相同的。 不使用平方损失的原因是，在使用 Sigmoid 函数作为正样本的概率时，同时将平方损失作为损失函数，这时所构造出来的损失函数是非凸的，不容易求解，容易得到其局部最优解。 而如果使用极大似然，其目标函数就是对数似然函数，该损失函数是关于未知参数的高阶连续可导的凸函数，便于求其全局最优解。

逻辑回归它假设样本服从伯努利分布(0-1分布)，进而求得满足该分布的似然函数，接着取对数求极值等，逻辑回归推导出的经验风险函数就是最小化负的似然函数，从损失函数的角度来看，就是对数损失函数。

## 2. 梯度消失和梯度爆炸的产生的原理以及常用解决方法是什么？

梯度爆炸现象：损失出现NAN，一般出现在深层网络中和权值初始值太大的情况；梯度消失现象：网络不学习，即参数不更新，一般出现在深层网络和采用了不合适的激活函数。

从深层网络角度来讲：对于深层网络的参数更新，需要梯度的反向传播，使用链式求导法则，当某一部分对激活函数求导大于1时，当层数增多时，梯度更新将以指数级增加，发生梯度爆炸。如果该部分小于1，那么随着层数增多，求出的梯度更新将会以指数级减小，发生梯度消失。总结：从深层网络的角度来看，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时候甚至训练了很久，前几层的权值和刚开始初始化的值差不多，因次，梯度消失和梯度爆炸的根本原因是反向传播的链式求导法则，属先天不足。

从激活函数角度来讲：激活函数的选择 如sigmoid。

[详解机器学习中的梯度消失、爆炸原因及其解决方法](https://blog.csdn.net/qq_25737169/article/details/78847691)

解决方法：

+ 预训练+微调
+ 梯度剪切、正则
+ 更换激活函数 ReLU LeakReLU等
+ batchnorm
  + 对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。
+ 残差结构

## 3. 生成式模型和判别式模型有什么区别？各自的工作原理是什么？

监督学习方法分为生成方法(生成式模型)和判别方法(判别式模型)：

+ 生成式模型：由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测模型，即$P(Y|X) = \frac{P(X,Y)}{P(X)}$，这样的方法之所以称为生成式模型，是因为模型表示了给定输入$X$产生输出$Y$的生成关系。典型的模型有：
  + 朴素贝叶斯
  + 隐马尔科夫模型(HMM)
  + 混合高斯模型
  + 贝叶斯网络
  + 马尔可夫随机场(MRF)
  + 深度信念网络(DBN)
+ 判别式模型：由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测模型。判别式模型关心的是对给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型有：
  + K近邻(KNN)
  + 感知机
  + 决策树
  + 逻辑回归
  + 最大熵模型
  + 支持向量机
  + 提升方法
  + 条件随机场
  + 线性回归
  + 线性判别分析
+ 生成式模型的特点：生成式模型可以还原出联合概率分布$P(X,Y)$，而判别式模型则不能，生成式模型的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型，当存在隐变量时，仍可以用生成式模型学习，此时判别式模型就不能用了。优点：(1)实际上带的信息要比判别式模型丰富，研究单类问题比判别式模型灵活性强;(2)模型可以通过增量学习得到;(3)生成模型可以应付隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。缺点：(1)学习过程比较复杂;(2)实践中多数情况下判别模型效果更好。[参考文章](https://blog.csdn.net/Yaphat/article/details/52574748)

+ 判别式模型的特点：判别式模型直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率更高，由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。优点：(1)分类边界更灵活，比使用纯概率方法或生成式模型得到的更高级;(2)准确率往往比生成式模型高;(3)不需要求解类别条件概率，所以允许我们对输入进行抽象(比如降维，定义特征等)，从而能够简化学习问题。缺点：(1)不能反映训练数据的本身特性。

## 4. $L_1$正则和$L_2$正则如何理解？有什么区别？为什么能解决过拟合问题？

$L_1$正则化指权值向量中各个元素的绝对值之和，$L_2$正则化是指权值向量中各个元素的平方和再求平方根。$L_1$正则会产生稀疏解，$L_2$正则会产生比较小的解。以二维为例，$L_1$正则化项和误差项的交点常出现在坐标轴上，是个菱形，$w_1$或$w_2$为0，即权值向量中有零值元素，而$L_2$正则化项与误差项的交点常出现在某个象限中，是个圆，$w_1$和$w_2$均非0。

从贝叶斯的角度来看：正则化是假设模型参数服从先验概率，即为模型参数添加先验，只是不同的正则化方法的先验分布是不一样的，$L_1$正则是拉普拉斯先验，而$L_2$正则是高斯先验。这样就规定了参数分布，使得模型的复杂度降低，对噪声与异常点的抗干扰性的能力增强，从而提高了模型的泛化能力。[机器学习防止欠拟合、过拟合方法](https://zhuanlan.zhihu.com/p/29707029)

Lasso回归：逻辑回归的基础上加了$L_1$正则。
岭回归：逻辑回归的基础上加了$L_2$正则。

## 5. 什么是过拟合,欠拟合，解决过拟合和欠拟合的方法有哪些？

过拟合(over-fitting)：过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型图对已知数据预测的很好，但对未知数据预测很差的现象(—来自李航的统计机器学习)，原因总结就是数据太少以及模型太复杂。

过拟合原因：

+ 数据层面：(1)数据量太少；(2)训练集和测试集分布不均匀；(3)数据不纯，包含大量的噪声，模型过度拟合噪声数据。
+ 模型层面：模型太复杂

解决欠拟合的方法：

+ 添加其它项，有时候模型出现欠拟合是因为特征项不够导致的，可以添加其他特征项来解决
+ 添加多项式特征。将线性模型增加二次项或三次项使模型的拟合能力变强
+ 增加模型的复杂度
+ 减少正则化参数

解决过拟合现象的方法：

+ 数据集扩增：(从数据源头采集更多数据，复制原有数据并加上随机噪声，重采样，根据当前数据估计分布，用分布产生数据)
+ 特征选择
+ 正则化
+ 早停(Early Stopping)
+ dropout
  + **减少神经元之间复杂的共适应关系**： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样**权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况**）。 **迫使网络去学习更加鲁棒的特征** （这些特征在其它的神经元的随机子集中也存在）。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。（这个角度看 dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高）
  + **蕴含模型集成的思想**。每次随机dropout一些神经元，相当于训练了多个不同的模型，最后在测试的时候是多个模型集成的结果。
+ 残差结构
+ 交叉验证

## 6. 有哪些评价指标？AUC怎么求？AUC刻画的是什么？Precision和Accuracy的区别？

|      | Yes           | No           | 总计       |
| ---- | ------------- | ------------ | ---------- |
| Yes  | TP            | FN           | P(实为Yes) |
| No   | FP            | TN           | N(实为No)  |
| 总计 | P'(被分为Yes) | N'(被分为No) | P+N        |

评价指标

+ **正确率(accuracy)**:$accuracy = \frac{(TP+TN)}{P+N}$,正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好。
+ **错误率(Error rate)**:错误率与正确率相反，描述被分类器错分的比例。
+ **灵敏度(sensitivity)**:$sensitivity=\frac{TP}{P}$,表示所有正例中被分对的比例，衡量了分类器对正类的识别能力。
+ **特异性(specificity)**:$specificity=\frac{TN}{N}$,表示所有负例中被分对的比例，衡量了分类器对负例的识别能力。
+ **精度(Precision)**:$precision=\frac{TP}{TP+FP}$,精度是精确性的度量，表示被分为正例的样本中实际为正例的比例。
+ **召回率(Recall)**:$recall=\frac{TP}{TP+FN}$,召回率是覆盖面的度量，度量多少个正例被分为正例，召回率和灵敏度一样。
+ **$F_1$score**:综合了查准率(精度)和查全率(召回率)，$F_1=\frac{2\times precision \times recall}{precision+recall}$.

AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。AUC越大，模型越可靠。

## 7. RF、XGBoost和GBDT的原理？RF和XGBoost/GBDT的区别？XGBoost和GBDT的区别?

RF，GBDT和XGBoost都属于集成学习(Ensemble Learning)。集成学习的目的是通过结合多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性。根据个体学习器的生成方式，可将目前的集成学习方法分为两大类：(1)个体学习器之间存在强依赖关系，必须串行生成的序列化方法(Boosting系列方法)；(2)个体学习器之间不存在强依赖关系，可同时生成的并行化方法(bagging,随机森林 RF)。

### RF(随机森林)原理

随机森林是Bagging(简单理解为：放回抽样(自主采样法)，多数表决(分类)或简单平均(回归)，其基学习器之间属于并列生成，不存在强依赖关系)的扩展变体，它以决策树为基学习器，构建Bagging集成的基础上进一步在决策树的训练过程中引入了**随机属性选择**。因此可包括四个部分：随机选择样本(自主采样法)、随机属性选择、构建决策树、随机森林投票(平均)。

随机选择样本与Bagging相同；随机属性选择是在构建树的过程中，先从样本集的特征集合中随机选择部分特征(属性)，然后再从这个子集中选择最优属性用于划分，这种随机性导致随机森林的偏差会有稍微的增加(相比于单棵非随机树)，但由于随机森林的“平均”性质，会使得它的方差减小，而方差减小补偿了偏差的增大，因此总体会得到更好的模型。在构建决策树的时候，RF的每棵决策树都最大可能的进行生长而不进行剪枝，在对预测输出进行结合时，RF对于分类任务采用投票，回归任务采样平均法。

**RF的重要特性是不对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每一个基学习器只使用了训练集中约63.2%的样本，剩余的样本可用作验证集来对其泛化性能进行“包外估计”。**

RF和Bagging比较：RF其实性能较差，特别是当只有一个基学习器的时候，随着学习器数目的增多，随机森林通常会收敛于更低的泛化误差。随机森林的训练效率也高于Bagging，因为在单个决策树的构建过程中Bagging使用的是“确定”决策树，即在选择特征划分结点时需要考虑所有的属性，而随机森林则是随机选取部分属性来构建决策树。

优点：

+ 简单，易实现，计算开销小，容易理解和解释，树可以被可视化；
+ 能够处理很高维的数据，并且不用特征选择，而且训练完成后，给出特征的重要性；
+ 隐含地创建了多个联合特征，并能够解决非线性问题；
+ 自带out-of-bag(包外估计)错误评估能力
+ 容易做成并行化方法

缺点：

+ 不适合小样本，只适合大样本；
+ 大多数情况下，RF模型的精度略低于GBDT模型的精度；
+ 适合决策边界是矩形的，不适合对角线型。

### GBDT(梯度提升树)

先介绍一下Boosting，它是也是一种集成方法，但与Bagging不同的是，不同的分类器是通过串行训练而获得的，每个分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。Boosting分类的结果是基于所有分类器的加权求和的结果，每个权重代表对应的分类器在上一轮迭代中的成功度，而Bagging是的分类器的权重是一样的。

GBDT与传统的Boosting区别较大，**它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们在残差减小的梯度方向上建立模型**，所以说，在GradientBoost(GB)中，**每个新的模型的建立都是为了使得之前的模型的残差往梯度方向下降的方法**。**在GB算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART树**。GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT树都是CART回归树，而不是分类树(尽管GBDT调整后也可以用于分类但不代表GBDT的树是分类树)。

优缺点：GBDT的性能在RF的基础上又有进一步的提升；它能灵活处理各种类型的数据；在相对较少的调参时间下，预测的准确度较高。由于其实Boosting，基学习器之间存在串行关系，难以并行训练数据。

### XGBoost原理

XGBoost是GB算法的高效实现，其在GBDT的基础上进一步提升，能够自动地应用CPU的多线程进行并行计算，同时在算法精度上也进行了精度的提高。

+ XGBoost中的基学习器除了可以是CART也可以是线性分类器。
+ **传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数**；
+ XGBoost在**代价函数中加入了正则项**，用于控制模型的复杂度，降低了模型的方差，防止过拟合；
+ 对缺失值的处理。对特征值有缺失的样本，XGBoost可以自动学习出它的分裂方向；
+ XGBoost支持并行；
+ 列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算量。

### GBDTT与RF的相同点

+ 都是由多棵树组成；
+ 最终的结果都是由多棵树一起决定。

### GBDT与RF的不同点

+ 组成随机森林的树可以是分类树，也可以是回归树，而GBDT只能是回归树；
+ 组成随机森林的树可以并行生成，而GBDT只能串行生成；
+ 对于最终的输出结果而言，随机森林采用多数投票等，而GBDT则是将所有结果累加起来，或者加权累加起来；
+ 随机森林对异常值不敏感，GBDT对异常值非常敏感；
+ 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器集成；
+ 随机森林是通过减少模型方差提高性能，GBDT是通过减少偏差提高性能。

## 8. SVM相关问题

### SVM的原理，求解过程，损失函数，核函数？

SVM的原理：

求解过程：

损失函数：

核函数：

### SVM使用对偶计算的目的是什么？

+ 转换对偶问题求解减少算法复杂度，使得算法更高效，从求解w,b转换成求解$\alpha$；
+ 不等式约束是优化问题中的难点，求解对偶问题可以将支持向量机原问题中的不等式约束转换成等式约束；
+ 支持向量机在解决非线性可分问题时，需要将数据映射到高维空间，但映射函数的具体形式不容易确定，而转换成对偶问题时，可以使用核函数来解决这个问题。

### SVM的主要缺点

+ SVM算法对大规模训练样本难以实施
+ 用SVM解决多分类问题存在困难
+ 对缺失数据敏感，对参数和核函数的选择敏感。

### 核函数的特点及其作用

**引入核函数的目的：**把原坐标系里线性不可分的数据用核函数投影另一个空间，尽量使得数据在新的空间里线性可分。
核函数的特点：

+ 核函数的引入避免了"维数灾难"，大大减少了计算量(输入空间的维数对核函数矩阵无影响，因此核函数可以有效处理高维输入)
+ 无需知道非线性变换函数$\Phi$的形式和参数
+ 核函数的形式和参数变换会隐式的改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。
+ 核函数可以和不同的算法结合，形成多种基于核函数技术的方法。

### SVM和逻辑回归的异同

相同点：

+ LR和SVM都是分类算法
+ LR和SVM都是监督学习算法
+ LR和SVM都是判别式模型
+ 如果不考虑核函数，LR和SVM都是线性分类算法，即他们的分类决策面是线性的。(注意：LR也可以核化，但是计算量太大，一般不这么做)

不同点：

+ LR采用-log损失(对数损失函数)，SVM采用合页损失函数(hinge)
+ LR对异常值敏感，SVM对异常值不敏感
+ 计算复杂度不同，对于海量数据，SVM效率较低，LR效率较高
+ 对于非线性问题的处理方式不同
+ SVM的损失自带正则
+ SVM自带结构风险最小化，LR则是经验风险最小化。
+ SVM一般会使用核函数，LR一般不使用核函数。

## 9. 常见的激活函数和损失函数

### 激活函数

+ Linear
+ ReLU
+ Sigmoid
+ Softmax
+ tanh
+ LeackReLU

### 损失函数

损失函数用来衡量算法运行时，估计模型的预测值和真实值的不一致程度，是一个非负实值函数，损失函数越小，模型的鲁棒性越好。损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是在经验风险损失函数的基础上加了正则化项。

+ 0-1损失函数
+ 绝对值损失函数
+ 平方损失函数
+ 对数损失函数
+ 指数损失函数
+ Hinge损失函数
+ 交叉熵损失函数
+ SmoothL1损失

## 10. 常见的特征选择算法和特征提取算法

子集搜索+子集评价=特征选择算法
子集搜索：前向搜索、后向搜索、双向搜索
子集评价：信息增益

+ 特征选择算法
  + Filter方法(过滤式方法)
    + 过滤式选择先对数据集进行特征选择，然后在训练学习器，特征选择过程与后序学习器无关，代表算法 Relief(相关系数)，信息增益(ID3),卡方检验；
  + wrapper方法(包裹式方法)
    + 包裹式特征选择直接把最终将使用的学习器性能作为特征子集的评价准则，其直接针对给定的学习器进行优化，一般性能比过滤式方法要好，但是计算开销较大，代表算法 LVW；
  + Embedded方法(嵌入式方法)
    + 嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，主要方法：正则化 $L_1$ Lasso回归(在线性回归的过程中加入了$L_1$正则项)。

+ 特征提取算法
  + PCA(主成分分析)
  + LDA(线性判别分析)
  + ICA(独立成分分析)

## 11. 常见的分类算法(传统)有哪些？

+ 逻辑回归
+ 决策树
+ SVM
+ 贝叶斯分类
+ K近邻
+ 线性判别分析
+ 提升方法
+ 感知机

## 12. DNN、CNN、RNN和LSTM的区别？



## 13. 优化算法有哪些？各有什么优势？

[各优化算法的优缺点整理](https://blog.csdn.net/zhouhong0284/article/details/80232412)

## 14. 当数据特征非常多相关的时候，哪些模型会失效？

1.朴素贝叶斯分类器模型 2.SVM 3.LR 4.决策树  答案：1，3。

个人理解：

+ 朴素贝叶斯分类器：采用"属性条件独立假设"，对已知类别，假设所有属性相互独立。换言之，假设每个属性独立地对分类结果发生影响。
+ SVM：针对线性不可分的情况，可以利用核函数将其映射到高维空间线性可分。
+ LR：其实质上是一个线性分类器，处理不好特征之间相关的情况。
+ 决策树：擅长处理非线性的问题。

### 15. 什么是BN？有什么作用？为什么BN会对学习率变的不敏感？

[深入理解Batch Normalization批标准化](https://www.cnblogs.com/guoyaohua/p/8724433.html)
[Batch Normalization原理与实战](https://zhuanlan.zhihu.com/p/34879333)
机器学习IID假设：就是假设训练数据和测试数据是满足相同分布的，而是在训练数据上获得的模型能够在测试集上获得好的性能一个基本保障。
**BN**：对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间的饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。
BN作用：就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同的分布。

**BN的优势：**

1. **BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度**
   + BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。
2. **BN使得模型对网络中参数不那么敏感，简化调参过程，使得网络学习更加稳定**
   + 当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。经过BN操作以后，权重的缩放值会被“抹去”，因此保证了输入数据分布稳定在一定范围内。因此，在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型不收敛的风险。
   + BN把输入弄到一个合理范围内了，也就会把模型的输出限制在一个合理的范围内，因此学习率大一点或者稍微小一点就不会再导致模型输出落到激活函数的饱和区之类的地方
3. **BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题**
   + 通过BN操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题。
4. **BN具有一定的正则化效果**
   + 在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。

### 16.有哪些特征归一化的方法，为什么要做归一化？

在机器学习领域中，不同评价指标（即特征向量中的不同特征就是所述的不同评价指标）往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，**特征归一化就是将所有的特征都统一到一个大致相同的数值区间内**。**为了消除指标之间的量纲影响，需要进行数据归一化处理，使得不同指标之间具有可比性。**

如果不进行归一化，那么由于特征向量中不同特征的取值相差较大，会导致目标函数变“扁”。这样在进行梯度下降的时候，梯度的方向就会偏离最小值的方向，走很多弯路，即训练时间过长。如果进行归一化以后，目标函数会呈现比较“圆”，这样训练速度大大加快，少走很多弯路。总结：归一化后加快了梯度下降法求解最优解的速度，还可以提高模型的精度。

常用的归一化方法：

+ **线性归一化(最大最小归一化)**：将原始数据进行线性变化，使结果映射到$[0,1]$区间.$x'=\frac{x-min(x)}{max(x)-min(x)}$。
+ **零均值归一化**：将原始数据映射到均值为0、标准差为1的分布上。$z=\frac{x-\mu}{\sigma}$

通过梯度下降法求解的模型通常需要归一化的，包括，线性回归，逻辑回归，支持向量机，神经网络等。但对于决策树则不用。

### 17.特征离散化？为什么要进行特征离散化？有哪些方法？

连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。

**特征的连续值在不同的区间的重要性是不一样的，所以希望连续特征在不同的区间有不同的权重**，**实现的方法就是对特征进行划分区间，每个区间为一个新的特征**。常用做法，就是先对特征进行排序，然后再按照等频离散化为N个区间。

+ 离散特征的增加和减少都很容易，易于模型的快速迭代；
+ 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
+ 特征离散化后，模型会更稳定。对于区间里面的特征具有鲁棒性；
+ 特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险。

离散化的方法：

### 18.样本不平衡问题？有哪些方法？

**样本不平衡**是指分类任务中不同类别的训练样本数量差别很大的情况。

常见的样本不平衡的解决方法：

+ **扩大数据集**：增加小类样本的数据。
+ **对大类样本数据欠采样**：可以利用集成学习的思想，将大类样本数据划分为若干个集合供不同的学习器使用，相当于对每个学习器都进行了欠采样，但全局不会丢失重要信息。
+ **对小类样本数据进行过采样**：可以对训练集进行插值来产生新的少类样本，对于图像的话，可以对图像进行一些操作来增大小样本数据，比如，翻转，颜色变化，平移，放大缩小等等。
+ **数据代价加权**：对小样本数据赋予较大的权值，降低大类样本的权值。
+ 转化问题思考角度：可以将小样本当作异常点，进行异常点检测。

### 其他小问题：

1. C++中指针和引用的区别？分别在什么场景下使用？
2. 数组和链表的区别和使用场景？
3. 快排和堆排序。
4. python中的lambda函数？字典是个什么样的数据结构？

11.讲讲LSTM和GRU的区别。

6. 假设有一坨数据，其中有一个特征是pv，有的样本该特征值可以达到几百万，有的则接近于零，这个该如何归一化？
答：用每个值除以总体最大值和最小值的差
7. 这样做会有什么问题？
8. 那有什么解决方法吗？

13.介绍一下随机梯度下降算法，它的随机性提现在哪里？
16.介绍了一下天猫海外的业务，扯到那道编程题，说一下解决思路
17.一个小时编程完成：
长标题变短标题
1，实现一个函数，输入长字符串（大于30个字符），输出短字符串（小于等于10个字符）
2，提供2个词典，一个为同义词词典，key为w，value为w的同义词；一个为词语重要度词典，key为w，value为w的重要度
3，需要基于重要度词典，实现简单的分词；可以网上搜索基于词典的分词代码
4，输出标题中的词语尽量保持原来的顺序
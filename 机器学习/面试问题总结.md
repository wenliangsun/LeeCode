# 机器学习问题总结

## 1. 逻辑回归为啥不用均方误差作为损失函数，而是通过最大似然估计来求参数的？

对于逻辑回归，这里所说的对数损失和极大似然是相同的。 不使用平方损失的原因是，在使用 Sigmoid 函数作为正样本的概率时，同时将平方损失作为损失函数，这时所构造出来的损失函数是非凸的，不容易求解，容易得到其局部最优解。 而如果使用极大似然，其目标函数就是对数似然函数，该损失函数是关于未知参数的高阶连续可导的凸函数，便于求其全局最优解。

逻辑回归它假设样本服从伯努利分布(0-1分布)，进而求得满足该分布的似然函数，接着取对数求极值等，逻辑回归推导出的经验风险函数就是最小化负的似然函数，从损失函数的角度来看，就是对数损失函数。

## 2. 梯度消失和梯度爆炸的产生的原理以及常用解决方法是什么？

梯度爆炸现象：损失出现NAN，一般出现在深层网络中和权值初始值太大的情况；梯度消失现象：网络不学习，即参数不更新，一般出现在深层网络和采用了不合适的激活函数。

从深层网络角度来讲：对于深层网络的参数更新，需要梯度的反向传播，使用链式求导法则，当某一部分对激活函数求导大于1时，当层数增多时，梯度更新将以指数级增加，发生梯度爆炸。如果该部分小于1，那么随着层数增多，求出的梯度更新将会以指数级减小，发生梯度消失。总结：从深层网络的角度来看，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时候甚至训练了很久，前几层的权值和刚开始初始化的值差不多，因次，梯度消失和梯度爆炸的根本原因是反向传播的链式求导法则，属先天不足。

从激活函数角度来讲：激活函数的选择 如sigmoid。

[详解机器学习中的梯度消失、爆炸原因及其解决方法](https://blog.csdn.net/qq_25737169/article/details/78847691)

解决方法：

+ 预训练+微调
+ 梯度剪切、正则
+ 更换激活函数 ReLU LeakReLU等
+ batchnorm
+ 残差结构

## 3. 生成式模型和判别式模型有什么区别？各自的工作原理是什么？

监督学习方法分为生成方法(生成式模型)和判别方法(判别式模型)：

+ 生成式模型：由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测模型，即$P(Y|X) = \frac{P(X,Y)}{P(X)}$，这样的方法之所以称为生成式模型，是因为模型表示了给定输入$X$产生输出$Y$的生成关系。典型的模型有：
  + 朴素贝叶斯
  + 隐马尔科夫模型(HMM)
  + 混合高斯模型
  + 贝叶斯网络
  + 马尔可夫随机场(MRF)
  + 深度信念网络(DBN)
+ 判别式模型：由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测模型。判别式模型关心的是对给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型有：
  + K近邻(KNN)
  + 感知机
  + 决策树
  + 逻辑回归
  + 最大熵模型
  + 支持向量机
  + 提升方法
  + 条件随机场
  + 线性回归
  + 线性判别分析
+ 生成式模型的特点：生成式模型可以还原出联合概率分布$P(X,Y)$，而判别式模型则不能，生成式模型的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型，当存在隐变量时，仍可以用生成式模型学习，此时判别式模型就不能用了。优点：(1)实际上带的信息要比判别式模型丰富，研究单类问题比判别式模型灵活性强;(2)模型可以通过增量学习得到;(3)生成模型可以应付隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。缺点：(1)学习过程比较复杂;(2)实践中多数情况下判别模型效果更好。[参考文章](https://blog.csdn.net/Yaphat/article/details/52574748)

+ 判别式模型的特点：判别式模型直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率更高，由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。优点：(1)分类边界更灵活，比使用纯概率方法或生成式模型得到的更高级;(2)准确率往往比生成式模型高;(3)不需要求解类别条件概率，所以允许我们对输入进行抽象(比如降维，定义特征等)，从而能够简化学习问题。缺点：(1)不能反映训练数据的本身特性。

## 4. $L_1$正则和$L_2$正则如何理解？有什么区别？为什么能解决过拟合问题？

$L_1$正则化指权值向量中各个元素的绝对值之和，$L_2$正则化是指权值向量中各个元素的平方和再求平方根。$L_1$正则会产生稀疏解，$L_2$正则会产生比较小的解。

从贝叶斯的角度来看：正则化是假设模型参数服从先验概率，即为模型参数添加先验，只是不同的正则化方法的先验分布是不一样的，$L_1$正则是拉普拉斯先验，而$L_2$正则是高斯先验。这样就规定了参数分布，使得模型的复杂度降低，对噪声与异常点的抗干扰性的能力增强，从而提高了模型的泛化能力。[机器学习防止欠拟合、过拟合方法](https://zhuanlan.zhihu.com/p/29707029)

Lasso回归：逻辑回归的基础上加了$L_1$正则。
岭回归：逻辑回归的基础上加了$L_2$正则。

## 5. 什么是过拟合,欠拟合，解决过拟合和欠拟合的方法有哪些？

过拟合(over-fitting)：过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型图对已知数据预测的很好，但对未知数据预测很差的现象(—来自李航的统计机器学习)，原因总结就是数据太少以及模型太复杂。

过拟合原因：

+ 数据层面：(1)数据量太少；(2)训练集和测试集分布不均匀；(3)数据不纯，包含大量的噪声，模型过度拟合噪声数据。
+ 模型层面：模型太复杂

解决欠拟合的方法：

+ 添加其它项，有时候模型出现欠拟合是因为特征项不够导致的，可以添加其他特征项来解决
+ 添加多项式特征。将线性模型增加二次项或三次项使模型的拟合能力变强
+ 增加模型的复杂度
+ 减少正则化参数

解决过拟合现象的方法：

+ 数据集扩增：(从数据源头采集更多数据，复制原有数据并加上随机噪声，重采样，根据当前数据估计分布，用分布产生数据)
+ 特征选择
+ 正则化
+ 早停(Early Stopping)
+ dropout
+ 残差结构
+ 交叉验证

## 6. 有哪些评价指标？AUC怎么求？AUC刻画的是什么？Precision和Accuracy的区别？

## 7. RF、XGBoost和GDBT的原理？RF和XGBoost/GDBT的区别？XGBoost和GDBT的区别?

RF，GDBT和XGBoost都属于集成学习(Ensemble Learning)。集成学习的目的是通过结合多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性。根据个体学习器的生成方式，可将目前的集成学习方法分为两大类：(1)个体学习器之间存在强依赖关系，必须串行生成的序列化方法(Boosting系列方法)；(2)个体学习器之间不存在强依赖关系，可同时生成的并行化方法(bagging,随机森林 RF)。

### RF(随机森林)原理

随机森林是Bagging(简单理解为：放回抽样(自主采样法)，多数表决(分类)或简单平均(回归)，其基学习器之间属于并列生成，不存在强依赖关系)的扩展变体，它以决策树为基学习器，构建Bagging集成的基础上进一步在决策树的训练过程中引入了随机属性选择。因此可包括四个部分：随机选择样本(自主采样法)、随机属性选择、构建决策树、随机森林投票(平均)。

随机选择样本与Bagging相同；随机属性选择是在构建树的过程中，先从样本集的特征集合中随机选择部分特征(属性)，然后再从这个子集中选择最优属性用于划分，这种随机性导致随机森林的偏差会有稍微的增加(相比于单棵非随机树)，但由于随机森林的“平均”性质，会使得它的方差减小，而方差减小补偿了偏差的增大，因此总体会得到更好的模型。在构建决策树的时候，RF的每棵决策树都最大可能的进行生长而不进行剪枝，在对预测输出进行结合时，RF对于分类任务采用投票，回归任务采样平均法。

**RF的重要特性是不对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以再内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每一个基学习器只使用了训练集中约63.2%的样本，剩余的样本可用作验证集来对其泛化性能进行“包外估计”。**

RF和Bagging比较：RF其实性能较差，特别是当只有一个基学习器的时候，随着学习器数目的增多，随机森林通常会收敛于更低的泛化误差。随机森林的训练效率也高于Bagging，因为在单个决策树的构建过程中Bagging使用的是“确定”决策树，即在选择特征划分结点时需要考虑所有的属性，而随机森林则是随机选取部分属性来构建决策树。

优点：

+ 简单，易实现，计算开销小，容易理解和解释，树可以被可视化；
+ 能够处理很高维的数据，并且不用特征选择，而且训练完成后，给出特征的重要性；
+ 隐含地创建了多个联合特征，并能够解决非线性问题；
+ 自带out-of-bag(包外估计)错误评估能力
+ 容易做成并行化方法

缺点：

+ 不适合小样本，只适合大样本；
+ 大多数情况下，RF模型的精度略低于GBDT模型的精度；
+ 适合决策边界是矩形的，不适合对角线型。

### GDBT(梯度提升树)

先介绍一下Boosting，它是也是一种集成方法，但与Bagging不同的是，不同的分类器是通过串行训练而获得的，每个分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。Boosting分类的结果是基于所有分类器的加权求和的结果，每个权重代表对应的分类器在上一轮迭代中的成功度，而Bagging是的分类器的权重是一样的。

GDBT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们在残差减小的梯度方向上建立模型，所以说，在GradientBoost(GB)中，每个新的模型的建立都是为了使得之前的模型的残差往梯度方向下降的方法。在GB算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART树。GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT树都是CART回归树，而不是分类树(尽管GBDT调整后也可以用于分类但不代表GBDT的树是分类树)。

优缺点：GBDT的性能在RF的基础上又有进一步的提升；它能灵活处理各种类型的数据；在相对较少的调参时间下，预测的准确度较高。由于其实Boosting，基学习器之间存在串行关系，难以并行训练数据。

### XGBoost原理

XGBoost是GB算法的高效实现，其在GBDT的基础上进一步提升，能够自动地应用CPU的多线程进行并行计算，同时在算法精度上也进行了精度的提高。

+ XGBoost中的基学习器除了可以是CART也可以是线性分类器。
+ 传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；
+ XGBoost在代价函数中加入了正则项，用于控制模型的复杂度，降低了模型的方差，防止过拟合；
+ 对缺失值的处理。对特征值有缺失的样本，XGBoost可以自动学习出它的分裂方向；
+ XGBoost支持并行；
+ 列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算量。

### GDBT与RF的相同点

+ 都是由多棵树组成；
+ 最终的结果都是由多棵树一起决定。

### GDBT与RF的不同点

+ 组成随机森林的树可以是分类树，也可以是回归树，而GBDT只能是回归树；
+ 组成随机森林的树可以并行生成，而GBDT只能串行生成；
+ 对于最终的输出结果而言，随机森楼采用多数投票等，而GBDT则是将所有结果累加起来，或者加权累加起来；
+ 随机森林对异常值不敏感，GBDT对异常值非常敏感；
+ 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器集成；
+ 随机森林是通过减少模型方差提高性能，GBDT是通过减少偏差提高性能。

## 8. SVM相关问题

### SVM使用对偶计算的目的是什么？
  
+ 转换对偶问题求解减少算法复杂度，使得算法更高效；
+ 不等式约束是优化问题中的难点，求解对偶问题可以将支持向量机原问题中的不等式约束转换成等式约束；
+ 支持向量机在解决非线性可分问题时，需要将数据映射到高维空间，但映射函数的具体形式不容易确定，而转换成对偶问题时，可以使用核函数来解决这个问题。

## 9. 常见的激活函数？损失函数？

### 激活函数

+ Linear
+ ReLU
+ Sigmoid
+ Softmax
+ tanh
+ LeackReLU

### 损失函数

损失函数用来衡量算法运行时，估计模型的预测值和真实值的不一致程度，是一个非负实值函数，损失函数越小，模型的鲁棒性越好。损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是在经验风险损失函数的基础上加了正则化项。

+ 0-1损失函数
+ 绝对值损失函数
+ 平方损失函数
+ 对数损失函数
+ 指数损失函数
+ Hinge损失函数
+ 交叉熵损失函数
+ SmoothL1损失

## 10. 常见的特征选择算法和特征提取算法

子集搜索+子集评价=特征选择算法
子集搜索：前向搜索、后向搜索、双向搜索
子集评价：信息增益

+ 特征选择算法
  + Filter方法(过滤式方法)
    + 过滤式选择先对数据集进行特征选择，然后在训练学习器，特征选择过程与后序学习器无关，代表算法 Relief(相关系数)，信息增益(ID3),卡方检验；
  + wrapper方法(包裹式方法)
    + 包裹式特征选择直接把最终将使用的学习器性能作为特征子集的评价准则，其直接针对给定的学习器进行优化，一般性能比过滤式方法要好，但是计算开销较大，代表算法 LVW；
  + Embedded方法(嵌入式方法)
    + 嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，主要方法：正则化 $L_1$ 岭回归(在线性回归的过程中加入了正则项)。

+ 特征提取算法
  + PCA(主成分分析)
  + LDA(线性判别分析)
  + ICA(独立成分分析)

## 11. 常见的分类算法(传统)有哪些？

+ 逻辑回归
+ 决策树
+ SVM
+ 贝叶斯分类
+ K近邻
+ 线性判别分析
+ 提升方法
+ 感知机

## 12. DNN、CNN和RNN的区别？

## 13. 优化算法有哪些，各有什么优势？

## 14. 当数据特征非常多相关的时候，哪些模型会失效？

1.朴素贝叶斯分类器模型 2.SVM 3.LR 4.决策树  答案：1，3。

个人理解：

+ 朴素贝叶斯分类器：采用"属性条件独立假设"，对已知类别，假设所有属性相互独立。换言之，假设每个属性独立地对分类结果发生影响。
+ SVM：针对线性不可分的情况，可以利用核函数将其映射到高维空间线性可分。
+ LR：其实质上是一个线性分类器，处理不好特征之间相关的情况。
+ 决策树：擅长处理非线性的问题。

### 15. 什么是BN？工作原理？
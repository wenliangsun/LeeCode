# 深度学习问题总结

## DNN、CNN、RNN和LSTM的区别？

**DNN(多层感知机模型)**：它是一种多层神经网络结构，层与层之间以全连接的方式连接。模型具有较强的数据表征能力，但因为是全连接网络，模型的参数较多，容易出现过拟合，梯度消失等问题。

**CNN(卷积神经网络)**：它也是一种多层级的神经网络，由卷积层，池化层和全连接层组成，卷积层的层与层之间是局部连接和权重共享的。权重共享减少了网络模型需要训练的参数。

**RNN(递归神经网络)**：针对CNN无法对时间序列上的变化进行建模的局限，为了适应对时序数据的处理，RNN应运而生，它关键之处在于当前网络的隐藏状态会保留先前的输入信息，用来做当前网络的输入。它的模型是时间维度上的深度模型，可以对时序数据建模。

LSTM：

## 卷积神经网络CNN

### 卷积神经网络的特点

局部连接，权值共享，池化操作，多层次结构

### 卷积层的作用

主要用于提取图像的特征，并且卷积核的权重是可以学习的，所以在神经网络，卷积操作突破传统滤波器的限制，根据目标函数提取出想要的特征。

### 池化层的作用

减小图像尺寸即数据降维，缓解过拟合，保持一定程度的旋转和平移不变性。MaxPooling能保证卷积神经网络在一定范围内平移特征能得到同样的激励，具有平移不变形。

### 填充和步长问题

设输出的高和宽分别为$n_h$和$n_w$，即输入的形状是$n_h\times n_w$，卷积核的窗口的形状是$k_h\times k_w$。<br>
1. 无padding，步长为1时，则输出的特征图的形状是$(n_h-k_h+1) \times (n_w-k_w+1)$.
2. 有padding,即在高的两侧一共填充$p_h$行,在宽的两侧一共填充$p_w$行, 且当步长为1时，则输出的特征图的形状是$(n_h-k_h+p_h+1)\times (n_w-k_w+p_w+1)$。在很多情况下，会设置$p_h=k_h-1$和$p_w = k_w-1$来使得输入和输出的特征图具有相同的高和宽。假设$k_h$是奇数，则通常会在高的两侧填充$p_h/2$。如果$k_h$是偶数，一种可能是在输入的顶端一侧填充$\lceil p_h/2 \rceil$，即上取整，而在底端一侧填充$\lfloor p_h/2 \rfloor$，即下取整。在宽的两侧填充同理。卷积神经网络中通常使用奇数高宽的卷积核，所以两端上的填充个数相等。
3. 有padding,且有步长不为1时，即设高上的步长为$s_h$，宽上的步长是$s_w$时，输出的形状为$\lfloor (n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor (n_w-k_w+p_w+s_w)/s_w\rfloor$。如果设置$p_h=k_h-1$和$p_w = k_w-1$，则此时的输出形状是简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步长整除，那么输出的形状是$(n_h/s_h)\times (n_w/s_w)$。

**总结就是：**

$$
\boldsymbol{(\lfloor (n_h-k_h+2\times p_h)/s_h\rfloor +1) \times (\lfloor (n_w-k_w+2\times p_w)/s_w\rfloor + 1)}
$$

### 转置卷积/反卷积(deconvolution)的输出大小

[PyTorch中的转置卷积详解](https://blog.csdn.net/w55100/article/details/106467776)

转置卷积的大小由卷积核大小与滑动步长决定，假设in是输入特征图大小，k是卷积核大小，s是滑动步长，out是输出特征图大小，则有：

$$
\boldsymbol{out = (in - 1) \times s + k}
$$

### 卷积神经网络中空洞卷积的作用是什么

空洞卷积也叫扩张卷积，在保持参数个数不变的情况下增大了卷积核的感受野，同时它可以保证输出的特征映射（feature map）的大小保持不变。一个扩张率为2的3×3卷积核，感受野与5×5的卷积核相同，但参数数量仅为9个。

### $1\times 1$卷积的作用

1. **实现跨通道的交互和信息整合**:NIN网络对不同通道的特征整合到MLP自网络中，让不同通道的特征能够交互整合，使通道之间的信息得以流通，其中的MLP子网络恰恰可以用的卷积进行代替。
2. **特征通道的升维和降维**：由于3X3卷积或者5X5卷积在几百个filter的卷积层上做卷积操作时相当耗时，所以1X1卷积在3X3卷积或者5X5卷积计算之前先降低维度。
3. **可以在保持特征图尺寸不变（即不损失分辨率）的前提下大幅增加非线性特性**：卷积层之后经过激励层，$1\times 1$的卷积在前一层的学习表示上添加了非线性激活，提升网络的表达能力。

### 感受野怎么计算？

感受野（Receptive Field），指的是神经网络中神经元“看到的”输入区域，在卷积神经网络中，feature map上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。感受野是个相对概念，某层feature map上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。

考虑从顶层到浅层的方式计算：

$$
F(i, j-1) = (F(i, j) - 1) \times stride + kernel_size
$$
其中 $F(i,j)$ 表示第i层对第j层的局部感受野。
算法代码(不计算padding):

```python
def inFromOut(net, layernum):
   RF = 1
   for layer in reversed(range(layernum)):
      fsize, stride, pad = net[layer]
      RF = (RF - 1) * stride + fsize
   return RF
```

$$
F(i, j-1) = (F(i, j) - 1) \times stride + dilation \times (kernel_size - 1) + 1
$$

算法代码：

```python
def outFromIn(isz, net, layernum):
    totstride = 1
    insize = isz
    for layer in range(layernum):
        fsize, stride, pad, dilation = net[layer]
        outsize = (insize - fsize + 2 * pad) / stride + 1
        insize = outsize
        totstride = totstride * stride
    return outsize, totstride
```

### 卷积神经网络参数量和计算量的计算。

计算量以计算机做乘加次数为单位，即完成某个操作，需要执行多少次乘法和加法操作。参数量以参数个数为单位，要计算内存或显存的，用参数量乘以每个参数所占的字节数即可。<br>
**参数量**：
CNN网络的参数量和特征图的尺寸无关，仅和卷积核的大小，偏置及BN有关。设卷积核为 $kernel = (C, K, S, O)$，则卷积层轻重的参数量为$C\times K \times K \times O$，偏置的参数量为$O$，如果使用了BN，那么还有两个可学习的参数$\alpha$，$\beta$，参数量都是$O$，总共$2\times O$。综上，该卷积层所有的参数量为：

$$
C \times K \times K \times O + 3 \times O
$$

需要注意的是，上面计算的仅仅是模型的参数量，若要计算模型实际需要多少显存，还要考虑特征图的大小，因为每一层卷积的输出都需要缓存，还要BN计算出来的均值和偏差也需要缓存，权重的梯度也需要缓存。通常模型参数所占用的显存比例很小。

**计算量**：
设输入特征图为$F = (B, W, H, C)$，$kernel = (C, K, S, O)$，B是batch size大小， H，W，C分别是输入特征图的高、宽和通道数，K是卷积核的大小，S是移动的步长，O是输出的特征图数量。
1. 一次卷积的计算量：一个$K \times K$的卷积，执行一次卷积操作，需要$K \times K$次乘法操作(卷积核中每个参数都要和特征图上的元素想成一次)，$K \times K - 1$次加法操作(将卷积结果加起来)，所以一次卷积操作需要计算量为：$$(K \times K) + (K \times K - 1) = 2 \times K \times K - 1$$
2. 在一个特征图上执行卷积需要的计算量：在一个特征图上需要执行的卷积次数$(\frac{H-K+P_h}{S} + 1) \times (\frac{H-K+P_w}{S}+1)$，若不能整除，则向下取整。则在一个特征图上的计算量为$$(\frac{H-K+P_h}{S} + 1) \times (\frac{H-K+P_w}{S}+1) \times (2 \times K \times K - 1)$$
3. C个特征图上进行卷积操作输出一个特征图的计算量：在C个输入特征图上进行卷积操作之后，需要将卷积的结果相加，得到一个输出特征图上的卷积结果，C个相加需要C-1次加法，所以输出一个特征图需要的计算量为：$$C \times (\frac{H-K+P_h}{S} + 1) \times (\frac{H-K+P_w}{S}+1) \times (2 \times K \times K - 1) + (C-1)$$
4. 输出O个特征图需要计算的次数:$$O \times (C \times (\frac{H-K+P_h}{S} + 1) \times (\frac{H-K+P_w}{S}+1) \times (2 \times K \times K - 1) + (C-1))$$
5. 一个batch的样本需要的计算量：$$B \times O \times (C \times (\frac{H-K+P_h}{S} + 1) \times (\frac{H-K+P_w}{S}+1) \times (2 \times K \times K - 1) + (C-1))$$

### 卷积核的大小(尺寸)为什么都是奇数的？这样有什么好处？

1. **更容易padding**：如果卷积核尺寸是奇数，就可以从图像的两边对称的padding。
2. **更容易找到卷积锚点，有几何中心方便确定位置**：保证了锚点刚好在中间，方便以模块中心为标准进行滑动卷积，避免了位置信息发生偏移。

### 卷积核是否一定越大越好？

早期的神经网络受限于计算能力和模型设计，无法将网络叠加的很深，因此卷积层需要设置较大的卷积核来获得较大的感受野，但是这种大卷积核会导致计算量大幅增加，后来发现堆叠2个$3\times 3$的卷积核可以获得与$5\times 5$对同的感受野，而且参数更少。所以，在大多数情况下，通过堆叠较小的卷积核比直接采用单个更大的卷积核更加有效。
在自然语言处理领域，特征提取只需要较浅的网络即可，但文本有时又需要较大的感受野让模型能组合更多的特征，此时增大卷积核是一个更好的选择。
卷积核的大小没有绝对的优劣，需要视具体应用场景来定，但是极大极小的卷积核都是不合适的。单独的$1\times 1$极小的卷积核只能用作分离卷积而不能对输入的原始特征进行有效组合，极大的卷积核通常会组合过多的无意义的特征造成计算资源的浪费。

### 为什么现在倾向于用小尺寸的卷积核？

用多个小卷积核串联可以有大卷积核同样的能力，而且参数更少，另外有更多次的激活函数作用，增强非线性。

### 怎么减少卷积层参数量？

1. 使用堆叠小卷积核来代替大卷积核：比如用2个$3\times 3$的卷积核代替1个$5\times 5$的卷积核。
2. 使用深度可分离卷积：将原本的$K\times K \times C$的卷积操作分离为$K\times K \times 1$和$1\times 1 \times C$的两部分操作
3. 添加$1\times 1$的卷积操作：在卷积操作之前进行通道降维
4. 在卷积层前使用池化操作。

### 简单叙述卷积神经网络的前向传播和反向传播过程

[卷积神经网络的前向传播](https://www.cnblogs.com/pinard/p/6489633.html)<br>

#### 反向传播

[卷积神经网络的反向传播](https://www.cnblogs.com/pinard/p/6494810.html)

**卷积层的反向传播**：见文件

**池化层的反向传播**：在反向传播时，我们首先会把梯度的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把梯度的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是Average，则把梯度的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做upsample。

## 激活函数相关

### 为什么需要非线性激活函数？

**为什么需要激活函数？**<br>
1. 激活函数对模型学习、理解复杂和非线性的函数具有重要作用。
2. **激活函数可以引入非线性因素**。解释：如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。
3. 激活函数可以把当前特征空间通过一定的非线性映射转换到另一个空间，让数据能够更好的被分类。

### 为什么激活函数需要非线性函数？

1. 假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。
2. 使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。

### 常见的激活函数

1. sigmoid激活函数：函数的定义为：$ f(x) = \frac{1}{1 + e^{-x}} $，其值域为 $ (0,1) $。
2. tanh激活函数：函数的定义为：$ f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $，值域为 $ (-1,1) $。
3. Relu激活函数：函数的定义为：$ f(x) = max(0, x) $  ，值域为 $ [0,+∞) $；
4. Leak Relu 激活函数 ： 函数定义为： $ f(x) =  \left\{
\begin{aligned}
ax, \quad x<0 \\
x, \quad x>0
\end{aligned}
\right. $，值域为 $ (-∞,+∞) $。
5. SoftPlus 激活函数： 函数的定义为：$ f(x) = ln( 1 + e^x) $，值域为 $ (0,+∞) $。
6. softmax激活函数：函数定义为： $ \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} $。多用于多分类神经网络的输出。

### 激活函数有哪些性质？

1. 非线性： 当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $ f(x)=x ​$，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；
2. 可微性： 当优化方法是基于梯度的时候，就体现了该性质；
3. 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；
4. $ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；
5. 输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。

### 如何选择激活函数？

​	选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。

以下是常见的选择情况：

1. 如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。
2. 如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。
3. sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。
4. tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。
5. ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。
6. 如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。

### 什么时候可以用线性激活函数？

1. 输出层，大多使用线性激活函数。
2. 在隐含层可能会使用一些线性激活函数。
3. 一般用到的线性激活函数很少。

### 使用ReLU激活函数有什么优点(相比于sigmoid激活函数)？

1. 在区间变动很大的情况下，ReLU激活函数的导数或者激活函数的斜率都会远大于0。在实现的时候就是一个if-else语句，而sigmoid的需要进行浮点四则运算，因此，在实践中，使用ReLU激活函数通常比sigmoid激活函数更快。
2. sigmoid激活函数(tanh激活函数)的导数在正负饱和区的时候，梯度会接近于0，造成梯度消失，而ReLU激活函数在大于0的部分的导数是一个常数，不会发生梯度消失现象。
3. ReLU激活函数进入负半区时，梯度为0，此时神经元不会训练，**产生所谓的稀疏性**。Leaky ReLU不会产生这个问题。

### 怎么理解ReLU(<0时)是非线性激活函数？

1. 单侧抑制
2. 相对宽阔的兴奋边界
3. 稀疏激活性：ReLU函数从图像上看，是一个分段线性函数，把所有的负值都变为0，而正值不变，这样就成为单侧抑制。因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x<0 $ 时，ReLU 硬饱和，而当 $ x>0 $ 时，则不存在饱和问题。ReLU 能够在 $ x>0 $ 时保持梯度不衰减，从而缓解梯度消失问题。

### 为什么tanh激活函数会比sigmoid激活函数的表现好？

1. 因为tanh激活函数是0均值的，更加有利于提高训练效率，而sigmoid的输出是在0-1之间的，总是正数，在训练过程中参数的梯度值是同一符号，这样优化路径容易出现zigzag（z字形）现象，不容易达到最优值。
2. tanh激活函数的导数区间是`[0,1]`，而sigmoid激活函数的导数区间是`[0,0.25]`，所以使用tanh激活函数会比sigmoid激活函数学习更快。

## Batch Size相关问题

### 在合理的范围内，增大batch size有何好处？

1. 内存利用率提高了，大矩阵乘法的并行化效率提高。
2. 跑完一次epoch（所有数据）所需要的迭代次数减少，对于相同数据量的处理速度进一步加快。
3. 在一定范围内，一般来说batch size越大，其确定的下降方向越准，引起训练震荡越小。

### 盲目增大batch size有何坏处？

1. 内存利用率提高了，但是内存容量可能撑不住。
2. 跑完一次epoch(所有数据)所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的更新也显得缓慢了
3. batch size增大到一定程度，其确定的下降方向已经基本不再改变。

### 调节batch size对训练效果影响到底如何？

1. batch size太小，模型表现效果极其糟糕。
2. 随着batch size的增大，处理相同数据量的速度越快。
3. 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
4. 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。
5. 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。

### 深度学习中的batch size的大小对训练过程的影响是什么样的？

[浅析深度学习中Batch Size大小对训练过程的影响](https://flashgene.com/archives/70920.html)<br>[深度学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260)<br>
由于目前主流深度学习框架处理mini-batch的反向传播时，默认都是先将每个mini-batch中每个instance得到的loss平均化之后再反求梯度，也就是说每次反向传播的梯度是对mini-batch中每个instance的梯度平均之后的结果，所以b的大小决定了相邻迭代之间的梯度平滑程度， b太小，相邻mini-batch间的差异相对过大，那幺相邻两次迭代的梯度震荡情况会比较严重，不利于收敛；<br>b越大，相邻mini-batch间的差异相对越小，虽然梯度震荡情况会比较小，一定程度上利于模型收敛，但如果b极端大，相邻mini-batch间的差异过小，相邻两个mini-batch的梯度没有区别了，整个训练过程就是沿着一个方向蹭蹭蹭往下走，很容易陷入到局部最小值出不来。

## 归一化相关问题

### 有哪些特征归一化的方法，为什么要做归一化？

在机器学习领域中，不同评价指标（即特征向量中的不同特征就是所述的不同评价指标）往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，**特征归一化就是将所有的特征都统一到一个大致相同的数值区间内**。**为了消除指标之间的量纲影响，需要进行数据归一化处理，使得不同指标之间具有可比性。还可以避免神经元饱和，即当激活函数的输入很大或者很小的时候，会落入到激活函数的饱和区域(sigmoid)，而在反向传播的过程中，激活函数饱和区域的梯度趋于0，会导致梯度消失的问题。**

如果不进行归一化，那么由于特征向量中不同特征的取值相差较大，会导致目标函数变“扁”。这样在进行梯度下降的时候，梯度的方向就会偏离最小值的方向，走很多弯路，即训练时间过长。如果进行归一化以后，目标函数会呈现比较“圆”，这样训练速度大大加快，少走很多弯路。总结：归一化后加快了梯度下降法求解最优解的速度，还可以提高模型的精度。

常用的归一化方法：

+ **线性归一化(最大最小归一化)**：将原始数据进行线性变化，使结果映射到$[0,1]$区间.$x'=\frac{x-min(x)}{max(x)-min(x)}$， 比较适用于数值比较集中的情况。
+ **零均值归一化**：将原始数据映射到均值为0、标准差为1的分布上。$z=\frac{x-\mu}{\sigma}$

通过梯度下降法求解的模型通常需要归一化的，包括，线性回归，逻辑回归，支持向量机，神经网络等。但对于决策树则不用。

### 什么是BN(批归一化)？有什么作用？为什么BN会对学习率变的不敏感？

[深入理解Batch Normalization批标准化](https://www.cnblogs.com/guoyaohua/p/8724433.html)
[Batch Normalization原理与实战](https://zhuanlan.zhihu.com/p/34879333)
**深度网络中每一层的输入分布都会因为前层的参数变化而改变，这会导致我们不得不采用更低的学习率以及更小心的初始化等方式，减慢了模型的训练。BN就是通过每一个mini batch的规范化解决这一问题。BN层能够允许我们使用更大的学习率，不那么关注参数的初始化，也可以为模型提供正则化的功能。**
因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）**随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近(饱和区)导致反向传播时低层神经网络的梯度消失。**

**BN作用**：BN就是通过一定的规范化手段，把每层神经网络输入的分布强行拉回到均值为0方差为1的标准正态分布，使得非线性函数的输入值落入到其比较敏感的区域，这样可以让梯度变大，避免梯度消失的问题，而且可以加快训练速度。

**BN**：对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间的饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入到比较敏感的区域，以此避免梯度消失问题。变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。<br>
**BN的优势：**<br>
1. **BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度**
   + BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。
2. **BN使得模型对网络中参数不那么敏感，简化调参过程，使得网络学习更加稳定**
   + 当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。**经过BN操作以后，权重的缩放值会被“抹去”，因此保证了输入数据分布稳定在一定范围内。因此，在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强**，此时我们可以设置较大的学习率而不用过于担心模型不收敛的风险。
   + BN把输入弄到一个合理范围内了，也就会把模型的输出限制在一个合理的范围内，因此学习率大一点或者稍微小一点就不会再导致模型输出落到激活函数的饱和区之类的地方
3. **BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题**
   + 通过BN操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题。
4. **BN具有一定的正则化效果**
   + 在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。

### 批归一化（BN）算法流程

下面给出 BN 算法在训练时的过程

输入：上一层输出结果 $ X = {x_1, x_2, ..., x_m} $，学习参数 $ \gamma, \beta $

算法流程：

1. 计算上一层输出数据的均值

$$
\mu_{B} = \frac{1}{m} \sum_{i=1}^m(x_i)
$$

其中，$ m $ 是此次训练样本 batch 的大小。

2. 计算上一层输出数据的标准差

$$
\sigma_{B}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{B})^2
$$

3. 归一化处理，得到

$$
\hat x_i = \frac{x_i - \mu_{B}}{\sqrt{\sigma_{B}^2} + \epsilon}
$$

其中 $ \epsilon ​$ 是为了避免分母为 0 而加进去的接近于 0 的很小值

4. 重构，**对经过上面归一化处理得到的数据进行重构**，得到

$$
y_i = \gamma \hat x_i + \beta
$$

其中，$ \gamma, \beta $ 为可学习参数。

注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值 $ \mu_{\beta} $ 和标准差 $ \sigma_{\beta}^2 $。此时，均值 $ \mu_{\beta} $ 是计算所有 batch $ \mu_{\beta} $ 值的平均值得到，标准差 $ \sigma_{\beta}^2 $ 采用每个batch $ \sigma_{\beta}^2 $  的无偏估计得到。

### BN在训练和预测时有什么差别？

BN层对每个通道对应的$N\times H \times W$的数据进行归一化，并进行变化重构，变化重构参数 $\gamma$ 和$\beta$是可学习的，首先对输入数据取均值和标准差，并进行归一化，$\epsilon$是防止除0的很小数值，然后对归一化后的数据进行变换重构。
**BN层在训练阶段，对每一批mini-batch的数据分别求均值与方差用于标准化，但在测试阶段，比如进行一个样本的预测，就并没有batch的概念，因此这一值使用的是全期训练数据的均值方差，也即模型训练到当前状态所有数据的均值方差，这个数据是在训练过程中通过移动平均法得到的**。对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，$\gamma$和$\beta$。

### BN训练时为什么不用全量训练集的均值和方差呢？

因为用全量训练集的均值和方差容易过拟合，对于BN，其实就是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上能够增加模型的鲁棒性，也会在一定程度上减少过拟合。
也正是因此，BN一般要求将训练集完全打乱，并用一个较大的batch值，否则，一个batch的数据无法较好得代表训练集的分布，会影响模型训练的效果。

## 梯度消失和梯度爆炸的产生的原理以及常用解决方法是什么？

梯度爆炸现象：损失出现NAN，一般出现在深层网络中和权值初始值太大的情况；梯度消失现象：网络不学习，即参数不更新，一般出现在深层网络和采用了不合适的激活函数。

从深层网络角度来讲：对于深层网络的参数更新，需要梯度的反向传播，使用链式求导法则，当某一部分对激活函数求导大于1时，当层数增多时，梯度更新将以指数级增加，发生梯度爆炸。如果该部分小于1，那么随着层数增多，求出的梯度更新将会以指数级减小，发生梯度消失。总结：从深层网络的角度来看，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时候甚至训练了很久，前几层的权值和刚开始初始化的值差不多，因次，梯度消失和梯度爆炸的根本原因是反向传播的链式求导法则，属先天不足。

从激活函数角度来讲：激活函数的选择 如sigmoid。

[详解机器学习中的梯度消失、爆炸原因及其解决方法](https://blog.csdn.net/qq_25737169/article/details/78847691)

解决方法<br>
1. 预训练+微调
2. 梯度剪切、正则
3. 更换激活函数 ReLU LeakReLU等
4. batchnorm 解释：BN就是通过一定的规范化手段，把每层神经网络输入的分布强行拉回到均值为0方差为1的标准正态分布，使得非线性函数的输入值落入到其比较敏感的区域，这样可以让梯度变大，避免梯度消失的问题，而且可以加快训练速度。
5. 残差结构

## 什么是过拟合,欠拟合，解决过拟合和欠拟合的方法有哪些？

过拟合(over-fitting)：过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型图对已知数据预测的很好，但对未知数据预测很差的现象(—来自李航的统计机器学习)，原因总结就是数据太少以及模型太复杂。

过拟合原因

1. 数据层面：(1)数据量太少；(2)训练集和测试集分布不均匀；(3)数据不纯，包含大量的噪声，模型过度拟合噪声数据。
2. 模型层面：模型太复杂

解决欠拟合的方法

1. 添加其它项，有时候模型出现欠拟合是因为特征项不够导致的，可以添加其他特征项来解决
2. 添加多项式特征。将线性模型增加二次项或三次项使模型的拟合能力变强
3. 增加模型的复杂度
4. 减少正则化参数

解决过拟合现象的方法

1. 数据集扩增：(从数据源头采集更多数据，复制原有数据并加上随机噪声，重采样，根据当前数据估计分布，用分布产生数据)
2. 特征选择
3. 正则化
4. 早停(Early Stopping)
5. dropout
6. 残差结构
7. 交叉验证

## Dropout 为什么能够防止过拟合？

1. **减少神经元之间复杂的共适应关系**： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样**权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况**）。 **迫使网络去学习更加鲁棒的特征** （这些特征在其它的神经元的随机子集中也存在）。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。（这个角度看 dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高）
2. **蕴含模型集成的思想**。每次随机dropout一些神经元，相当于训练了多个不同的模型，最后在测试的时候是多个模型集成的结果。

## BN和dropout之间的冲突？

Dropout与BN之间冲突的关键是网络状态切换过程中存在神经方差的不一致行为。当网络从训练转为测试时，Dropout可以通过其随机失活保留率（即 p）来缩放响应，并在学习中改变神经元的方差，而BN仍然维持X的统计滑动方差。这种方差不匹配可能导致数值不稳定。而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能。

解决方法：一个是在所有 BN 层后使用 Dropout，另一个就是修改 Dropout 的公式让它对方差并不那么敏感，就是高斯Dropout。

[BN和Dropout共同使用时会出现的问题](https://blog.csdn.net/weixin_37947156/article/details/98763993)

## 深度学习中的常用的损失函数

### 分类损失函数

(1) 0-1损失函数：当标签与预测类别相等时，loss为0，否则为1。
$$
L(Y, f(x)) =
\begin{cases}
1,& Y\ne f(x)\\
0,& Y = f(x)
\end{cases}
$$
(2) 交叉熵损失函数或对数损失函数：

$$
L(y, p) = -\sum_i(y_ilog(p_i))
$$

(3) softmax损失函数：

$$
L(y_i) = -\sum_i \frac{e^i}{\sum_je^j}
$$

(4) KL散度：KL散度用于估计两个分布的相似性。$D_{kl}$是非负的，只有当p与q处处相等时，才会等于0。KL散度常用语生成式模型。

$$
D_{kl}(p|q) = \sum_ip_ilog(\frac{p_i}{q_i})\\
D_{kl}(p|q) = \sum_ilog(p_i) - log(q_i) = -l(p,p) + l(p,q)
$$

其中$l(p,p)$是分布p的熵，$l(p,q)$是p和q的交叉熵。

(5) Hinge损失函数或合页损失函数(SVM)：

$$
L(y) = \max{(0, 1-ty)}\\
L(Y, f(x)) = \max{(0, Yf(x))}
$$

其中y是预测值，范围是`(-1,1)`，t是目标值，其值为-1或1。

(6) 指数损失函数：它的特点就是梯度比较大，主要用于Adaboost集成学习算法中

$$
L(Y, f(x)) = \exp(-Yf(x))
$$

### 回归损失函数

(1) $L_1$损失函数：Mean absolute loss(MAE)也被称为$L_1$ Loss，是以绝对误差作为距离

$$
MAE = \frac{1}{n}\sum_i^n|(y_i-\hat{y_i})|
$$

由于**L1 loss具有稀疏性**，为了惩罚较大的值，因此常常将其作为正则项添加到其他loss中作为约束。**L1 loss的最大问题是梯度在零点不平滑，导致会跳过极小值**。

(2) $L_2$损失函数：Mean Squared Loss/ Quadratic Loss(MSE loss)也被称为L2 loss，或欧氏距离，它以误差的平方和作为距离

$$
MSE = \frac{1}{n}\sum_i^n(y_i-\hat{y_i})^2
$$

L2 loss也常常作为正则项。**当预测值与目标值相差很大时, 梯度容易爆炸**，因为梯度里包含了x−t。

(3) SmoothL1损失函数：在x比较小时，下式等价于L2 loss，保持平滑。在x比较大时，下式等价于L1 loss，可以限制数值的大小。

$$
SmoothL_1(x)=\left\{
\begin{aligned}
0.5x^2 if |x| < 1 \\
|x| - 0.5 其他
\end{aligned}
\right.
$$

## 数据增强

深层神经网络需要大量的训练数据才能获得较理想的结果，在数据量有限的情况下，可以通过数据增强来增加训练样本多样性，提高模型的鲁棒性避免过拟合。数据增强可以分为两大类，离线增强和在线增强。

**离线增强**：直接对数据集进行处理，数据的数目会变成增强因子乘以原数据集数目，用于数据集小的时候。
**在线增强**：获得batch数据后，对这个batch数据进行数据增强，如旋转、平移、翻转等变换，常用于大的数据集。
**具体变换**：

1. 旋转：可通过在原图上
2. 翻转变换：沿水平或垂直方向翻转图像
3. 缩放变换：按一定的比例放大或缩小图像
4. 平移变换：在图像平面上对图像以一定方式进行平移，沿水平或竖直方向进行平移
5. 尺度变换：对图像按照尺度因子进行放大或缩小
6. 对比度变换：在图像HSV颜色空间，改变饱和度S和亮度V分量，保持色调H不变，增加光照变化
7. 噪声扰动：对图像每个像素RGB随机扰动，常用噪声模式是椒盐噪声和高斯噪声。
8. 颜色变换：在训练集像素值的RGB颜色空间进行PCA
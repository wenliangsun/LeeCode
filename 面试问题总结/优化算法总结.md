# 优化算法总结

[各优化算法的优缺点整理](https://blog.csdn.net/zhouhong0284/article/details/80232412)

## 梯度下降算法

### 一维梯度下降算法

$$
\boldsymbol {x_t=x_{t-1}-\eta f'(x)}
$$

### 梯度下降算法可以降低目标函数值的原因

假设连续可导的函数的输入输出都是标量。给定绝对值足够小的数$\epsilon$，根据泰勒展开公式，可以得到下面的近似

$$
\boldsymbol {f(x+\epsilon )\approx f(x)+f'(x)\cdot \epsilon}
$$

这里$f'(x)$是f在x点处的梯度，一维函数的梯度是一个标量，即导数。
接下来，找一个常数$\eta > 0$，使得$|\eta f'(x)|$足够小，那么可以将$\epsilon$替换为$-\eta f'(x)$得到

$$
\boldsymbol {f(x-\eta f'(x)) \approx f(x) - \eta f'(x)^2}
$$

如果导数$f'(x)\neq 0$，那么$\eta f'(x)^2>0$，所以

$$
\boldsymbol {f(x-\eta f'(x)) \leq f(x)}
$$

这就意味着，如果通过$x \larr x - \eta f'(x)$来迭代x，函数$f(x)$的值可能会降低。因此在梯度下降中，我们先选取一个初始值$x$和常数$\eta > 0$，然后不断通过上式迭代$x$，直到达到停止条件。

### 超参数学习率的设置(为什么不能太小，也不能太大)

上面的式子中用到了$\eta$，通常称其为学习率，这是一个超参数，需要人工设定，如果使用过小的学习率，会导致$x$更新缓慢从而需要更多的迭代次数才能得到较好的解。**如果使用过大的学习率，$|\eta f'(x)|$可能过大从而使得前面提到的一阶泰勒展开公式不再成立，这时，我们无法保证迭代$x$会降低$f(x)$的值**。

### 多维梯度下降算法

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\SGD.png)

批梯度下降算法会获得全局最优解，缺点是每次更新每个参数时需要遍历所有的数据，计算量很大，并且会有很多的冗余计算，导致的结果是，当数据量很大时，每个参数的更新都很慢。

### 随机梯度下降

在深度学习⾥，⽬标函数通常是训练数据集中有关各个样本的损失函数的平均。设$f_i(x)$是有关索引为i的训练数据样本的损失函数，n是训练数据样本数，x是模型的参数向量，那么⽬标函数定义为

$$
\boldsymbol {f(x)=\frac{1}{n}\sum_{i=1}^{n}f_i(x)}
$$

目标函数在$x$处的梯度计算为

$$
\boldsymbol {\nabla f(x)=\frac{1}{n}\sum_{i=1}^n\nabla f_i(x)}
$$

如果使用梯度下降算法，每次自变量迭代的计算开销是$O(n)$，它随$n$线性增长，因此，当训练数据样本数很大时，梯度下降算法的计算开销是很大的。

随机梯度下降算法(SGD)减少了每次迭代的计算开销，在随机梯度下降的每次迭代过程中，我们随机均匀采样一个样本索引$i\in {1,2,...n}$，并计算梯度$\Delta f_i(x)$来迭代$x$

$$
\boldsymbol {x = x -\eta \nabla f_i(x)}
$$
可以看到每次迭代的计算开销从梯度下降的$O(n)$降到了常数$O(1)$，值得强调的是，随机梯度$\nabla f_i(x)$是对梯度$\nabla f(x)$的无偏估计

$$
\boldsymbol {E_i\Delta f_i(x) = \frac{1}{n} \sum_{i=1}^n\Delta f_i(x) = \Delta f(x)}
$$

这意味着，平均来说，随机梯度是对梯度的⼀个良好的估计。

随机梯度下降算法是以高方差频繁更新参数，优点是使得SGD会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。

### 小批量随机梯度下降

小批量随机梯度下降算法结合了随机梯度下降算法和批梯度下降算法的优点，每次更新的时候使用$B$个样本，减少了参数更新的次数，可以达到更加稳定的收敛结果。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\BSGD.png)

小批量随机梯度下降中每次迭代的计算开销为$O(|B|)$。当批量⼤小为1时，该算法即为随机梯度下降；当批量⼤小等于训练数据样本数时，该算法即为梯度下降。当批量较小时，每次迭代中使⽤的样本少，这会导致并⾏处理和内存使⽤效率变低。这使得在计算同样数⽬样本的情况下⽐使⽤更⼤批量时所花时间更多。当批量较⼤时，每个小批量梯度⾥可能含有更多的冗余信息。为了得到较好的解，批量较⼤时⽐批量较小时需要计算的样本数⽬可能更多，例如增⼤迭代周期数。

## 动量法

### 梯度下降算法的问题

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\动量法1.png)

###  动量法的更新公式

$$
\boldsymbol {v_t := \gamma v_{t-1}+ \eta_{t}g_{t}}
$$
$$
\boldsymbol {x_t := x_{t-1}-v_t}
$$

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\动量法.png)

### 由指数加权平均理解动量法

首先对动量法的速度变量做变形：

$$
\boldsymbol {v_t \larr \gamma v_{t-1}+(1-\gamma) (\frac{\eta_t}{1-\gamma}g_t)}
$$

由指数加权平均的形式可得，速度变量$v_t$实际上对序列$\{\eta_{t-1}g_{t-1}/(1-\gamma):i=0,...,1/(1-\gamma)-1\}$做了指数加权平均。换句话说，相比于随机梯度下降算法，动量法在每个时间步的自变量更新量近似于将前者对应的最近$1/(1-\gamma)$个时间步的更新量做了指数加权移动平均后再除以$1-\gamma$。所以，在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致。

## AdaGrad算法

在之前介绍过的优化算法中，⽬标函数⾃变量的每⼀个元素在相同时间步都使⽤同⼀个学习率来⾃我迭代。AdaGrad算法，它根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题.

$$
\boldsymbol {s_t := s_{t-1}+g_t \bigodot g_t}
$$
$$
\boldsymbol {x_t := x_{t-1} - \frac{\eta}{\sqrt{s_t+\epsilon}}\bigodot g_t }
$$

其中$\bigodot$是按元素相乘，$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，这⾥开⽅、除法和乘法的运算都是按元素运算的。这些按元素运算使得⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率。

特点:<br>
需要强调的是，小批量随机梯度按元素平⽅的累加变量$s_t$出现在学习率的分⺟项中。因此，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；反之，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢。然而，由于$s_t$⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的学习率在迭代过程中⼀直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。

## RMSProp算法

为了解决Adagrad算法中的问题，RMSProp算法对AdaGrad算法做了⼀点小小的修改。不同于AdaGrad算法⾥状态变量$s_t$是截⾄时间步t所有小批量随机梯度$g_t$按元素平⽅和，RMSProp算法将这些梯度按元素平⽅做指数加权移动平均。

$$
\boldsymbol {S_t := \gamma S_{t-1} + (1-\gamma)g_t \bigodot g_t}
$$
$$
\boldsymbol {X_t := X_{t-1}-\frac{\eta}{\sqrt{S_t+\epsilon}}\bigodot g_t}
$$

其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数 。因为RMSProp算法的状态变量$s_t$是对平⽅项$g_t\bigodot g_t$的指数加权移动平均，所以可以看作是最近$1/(1-\gamma )$个时间步的小批量随机梯度平⽅项的加权平均。如此⼀来，⾃变量每个元素的学习率在迭代过程中就不再⼀直降低（或不变）。

## AdaDelta 算法

除了RMSProp算法以外，另⼀个常⽤优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有⽤解的问题做了改进。有意思的是，AdaDelta算法没有学习率这⼀超参数。
AdaDelta算法也像RMSProp算法⼀样，使⽤了小批量随机梯度$g_t$按元素平⽅的指数加权移动平均变量$s_t$。与RMSProp算法不同的是，AdaDelta算法还维护⼀个额外的状态变量$\Delta x_t$，其元素同样在时间步0时被初始化为0。我们使⽤$\Delta x_{t-1}$来计算⾃变量的变化量。最后，我们使⽤$\Delta x_t$来记录⾃变量变化量$g_t'$按元素平⽅的指数加权移动平均。
$$
\boldsymbol {S_t := \rho S_{t-1}+(1-\rho)g_t\bigodot g_t}
$$
$$
\boldsymbol {g_t' := \sqrt{\frac{\Delta X_{t-1}+\epsilon}{S_t+\epsilon}}\bigodot g_t}
$$
$$
\boldsymbol {X_t := X_{t-1}-g_t'}
$$
$$
\boldsymbol {\Delta X_t := \rho \Delta X_{t-1}+(1-\rho)g_t'\bigodot g_t'}
$$

可以看到，如不考虑$\epsilon$的影响，AdaDelta算法跟RMSProp算法的不同之处在于使⽤$\sqrt{\Delta X_{t-1}}$来替代超参数$\eta$。

## Adam算法

Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。Adam算法使⽤了动量变量$v_t$和RMSProp算法中小批量随机梯度按元素平⽅的指数加权移动平均变量$s_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0\leq \beta_1 \le 1$（算法作者建议设为0.9），时间步t的动量变量$v_t$即小批量随机梯度$g_t$的指数加权移动平均。

$$
\boldsymbol {v_t \larr \beta_1v_{t-1} + (1-\beta_1)g_t}
$$

和RMSProp算法中⼀样，给定超参数$0 \leq \beta_2\le 1$（算法作者建议设为0.999），将小批量随机梯度按元素平⽅后的项$g_t\bigodot g_t$做指数加权移动平均得到$s_t$。

$$
\boldsymbol {s_t \larr \beta_2s_{t-1} + (1-\beta_2)g_t\bigodot g_t}
$$

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\adam.png)


### Adam的更新公式

$$
\boldsymbol {t:=t+1}
$$
$$
\boldsymbol {v_t := \beta_1v_{t-1}+(1-\beta_1)g_t}
$$
$$
\boldsymbol {s_t := \beta_2s_{t-1}+(1-\beta_2)g_t\bigodot g_t}
$$
$$
\boldsymbol {\hat{v_t} := \frac{v_t}{1-\beta_1^t}}
$$
$$
\boldsymbol {\hat{s_t} := \frac{s_t}{1-\beta_2^t}}
$$
$$
\boldsymbol {g_t'= \frac{\eta \hat{v_t}}{\sqrt{\hat{s_t}}+\epsilon}}
$$
$$
\boldsymbol {x_t := x_{t-1}-g_t'}
$$


# 目标检测相关问题总结

[算法目标检测面经](https://blog.csdn.net/qq_39706357/article/details/89947026)

目标检测任务：是找出图像中所有感兴趣的而目标(物体)，确定他们的位置和大小，是机器视觉领域的核心问题之一。

## Faster R-CNN相关问题

[一文读懂Faster RCNN](https://zhuanlan.zhihu.com/p/31426458)

### Faster R-CNN算法流程

1. 首先将图片经过CNN卷积神经网络提取特征图，特征图被共享用于后续RPN层和第二阶段的全连接层。
2. 将特征图输入RPN网络中，RPN网络生成Region Proposals。RPN会做两件事
   + 通过二分类来判断产生的anchor是否是物体；
   + 利用边界框回归分支修正anchor来获得更精确的proposals。
3. 将RPN产生的proposals和第一步产生的特征图一起送入RoIPooling层。RoIPooling的工作原理：
   + 从特征图中获取与proposals相对应的区域;
   + 将该区域划分为固定数量的子窗口, $7\times 7$;
   + 在这些子窗口上执行最大池化以提供固定大小的输出。RoIPooling层的输出大小为(N, 7, 7, 512)，其中N是RPN输出的proposals。
4. 对RoIPooling输出的特征进行分类，和边界框回归获得检测框最终的精确位置。

### Faster R-CNN的损失函数是什么？

分类损失：RPN采用二分类的交叉熵损失函数，Fast R-CNN采用多分类的交叉熵损失函数。

$$
L_{cls} = 交叉熵/softmax
$$

回归损失函数：RPN回归的是预测框和真实边界框(gt)之间的偏移量，训练时候偏移量的gt是anchor和真实边界框(gt)之间的偏移量。Fast R-CNN回归的是预测框和真实边界框(gr)之间的偏移量，训练的时候偏移量的gt是proposal和真实边界框(gt)之间的偏移量。使用Smooth L1损失函数来回归位置。

$$
SmoothL_1(x)=\left\{
\begin{aligned}
0.5x^2 if |x| < 1 \\
|x| - 0.5 其他
\end{aligned}
\right.
$$

### 使用Smooth L1损失函数的原因？

对于边框的预测是一个回归问题。通常可以选择平方损失函数（L2损失）$f(x)=x^2$。但这个损失对于比较大的误差的惩罚很高，即**对异常值很敏感**。我们可以采用稍微缓和一点绝对损失函数（L1损失）$f(x)=|x|$，它是随着误差线性增长，而不是平方增长。但这个**函数在0点处导数不存在**，因此可能会影响收敛。一个通常的解决办法是，分段函数，在0点附近使用平方函数使得它更加平滑。它被称之为Smooth L1损失函数。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\smoothl1.png)

### Faster R-CNN怎么筛选正负anchor？

为了训练RPN，我们需要为每个anchor分配一个二值类别标签(是目标和不是目标)，满足以下两个条件的anchor被设置为正标签：(1) 具有与实际边界框(gt)的重叠最高交并比(IoU)的anchor；(2)具有与实际边界框的重叠交并比超过0.7的anchor。如果与实际边界框的重叠交并比小于0.3的anchor，被设置为负标签。IoU在0.3到0.7之间的anchor不参与训练，即在计算损失的时候不计算在内。

在训练的时候，对于每一张图片，随机选取256个anchor(包含128个正anchor和128个负anchor)，如果正的anchor不够128个，则用负的anchor去填补。

### RPN的原理以及作用是什么？

RPN的提出是为了替代传统的候选框提取算法selective search的，selective search是比较传统的方法，比较耗时，而且网络达不到端到端训练与测试的效果。RPN的提出可以共享CNN提取出的特征图，节省是计算开销。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RPN.png)

**原理**：
假定已经通过卷积神经网络CNN得到了共享的特征图，假设他的大小是$N \times 16 \times 16$，然后我们进入RPN阶段，首先经过一个$3 \times 3$的卷积，得到一个$256 \times 16 \times 16$的特征图，也可以看作$16 \times 16$个256维特征向量，然后经过两次$1 \times 1$的卷积，分别得到一个$18 \times 16 \times 16$的特征图，和一个$36 \times 16 \times 16$的特征图，也就是$16 \times 16 \times 9$个结果($16\times 16$大的特征图，每个位置9个anchor)，每个结果包含2个分数和4个坐标，再结合预先定义的anchor，经过后处理，就得到候选框proposal。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RPN2.png)

## RoIPooling、RoIAlign和RRoIPooling的区别？

从feature map上经过RPN得到一系列的proposals，大概2k个，这些bbox大小不等，如何将这些bbox的特征进行统一表示就变成了一个问题。RoIPooling就是将这些不同大小的proposals通过RoI池化操作获得固定大小的子特征图，常见RoI池化操作有RoIPooling，ROIAlign，RROIPooling。

### RoIPooling

根据输入的图像，将RoI映射到feature map对应位置上，然后将映射后的区域划分为固定数量的块(bin)，然后在每一个块(bin)中做maxpooling，举例如下图:

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RoIPooling1.jpg)

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RoIPooling2.jpg)

**缺点**：
RoIPooling 需要取整，这样的取整操作进行了两次，**一次是得到proposal在feature map上映射时，一次是对映射后的区域划分同等大小的块(bin)时**。这样两次取整操作，会对后面的回归定位产生影响，尤其是小目标。

### ROIAlign

RoIAlign为了解决RoIPooling在两次取整时损失精度导致特征与候选框不对齐的问题，它首先基于划分固定数量的bin，在每个bin中取了固定的4个点(作者实验后发现取4效果较好)，也就是图二右侧的蓝色点，然后针对每一个蓝点，通过距离它最近的4个真实像素点的值加权(双线性插值)求得这个蓝点的值，一个bin内会算出4个新值，最终在这些新值中取max，作为这个bin的输出值，即可得到RoIAlign池化的结果。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RoIAlign.jpg)

### RRoIPooling

旋转框RRPN得到的候选区域是旋转矩形，而传统的RoI池化只能处理与坐标轴平行的候选区域，因此作者提出了RRoI Pooling用于RRPN中的旋转矩形的池化。首先需要设置超参数 $H_r$ 和 $W_r$，$H_r,W_r$分别表示池化后得到的子特征图的高和宽。然后将RRPN得到的候选区域等分成$H_r \times W_r$ 个小区域，接着**通过仿射变换将子区域转换成平行于坐标轴的矩形**，最后通过Max Pooling得到长度固定的特征向量。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RROI1.jpg)

需要对有角度的roi区域仿射变换到与坐标轴对齐的矩形数组中,再采用max pooling得到相同尺寸的特征图。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RROI1.jpg)

RROI Pooling的计算流程如算法2的伪代码。其中第一层for循环是遍历候选区域的所有子区域，5-7行通过仿射变换将子区域转换成标准矩形，第二个for循环用于取得每个子区域的最大值，10-11行由于对标准矩形中元素的插值，使用了向下取整的方式。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RROI3.jpg)

## Mask R-CNN相关问题

Mask R-CNN是一个实例分割的算法，主要是在目标检测框架的基础上又加了一个分支，用于对目标进行语义分割的操作，也是基于Faster R-CNN来改进的，它的主要贡献就是发现了Faster R-CNN框架中RoIPooling时的问题，也就是RoIPooling的两次取整操作导致映射的候选框区域与实际的候选框语义不对齐的问题，这个偏差会影响检测或分割的精度，尤其是对于小目标，Mask R-CNN基于此提出了ROIAlign的操作，来改进RoIPooling中的问题。

## FPN相关问题

### FPN解决了什么问题？

在以往的Faster R-CNN进行目标检测的时候，都是将proposals(候选框)映射到最后一个特征图上来提取候选框的特征，这对大目标的检测没有问题，但对于小目标的检测就不友好，因为对于小目标来说，当将候选框映射到最后一层的特征图上时，映射的区域很小，可能导致小目标的信息丢失，使得对小目标的检测性能很差。为了解决多尺度检测的问题，就提出了FPN的结构。

### FPN的创新点？

1. **多层级特征：**FPN生成不同层级的特征图分别用于检测不同尺度的目标，对于浅层的特征图，常用来检测比较小的目标，而对于较深层级的特征图，常用来检测比较大的目标，FPN利用了CNN的不同stage的特征图，融合出了不同层级的特征图，共享了前面的卷积计算，大大减少了计算量。
2. **特征融合：**之前的一些研究发现，浅层的特征图通常包含精确的定位信息，但是由于网络较浅，感受野较小导致提取到的高级语义信息很少，而高级语义信息对于目标的分类非常有用（比如在ImageNet上的一些网络后面越做越深），因此，浅层的特征图上检测通常是分类不准但是定位准确。FPN提出了自顶向下和横向连接的操作来缓解这一现象。它将后层的特征先进行上采样，采样成和前一层相同的大小，然后和前一层的特征图进行融合来获得一个融合高级语义信息的特征图，然后将当前层的特征图上采样继续向前传，再和对应特征图进行融合，得到特征金字塔。**改进(PANet)**：PANet发现，对于高层级的特征图，由于网络很深，经过了较多的卷积和池化层导致位置信息丢失，使得高层级特征图检测的位置不准确，它就从浅层引入了一个自底向上增强(Bottom-Up Path Augmentation)的路径，缩短了定位信息传递到高层特征图的路径，进而加强了高层层级特征图的位置信息。
3. 采用的是P2,P3,P4,P5层的特征图，分别下采样4，8，16，32倍。

## YOLO相关问题

### YOLOv1

**原理**：

YOLOv1将目标检测任务转变为一个回归任务，将整张图片作为输入，直接在输出层回归bounding box的位置及其所属类别。具体的：YOLO的CNN网络将输入的图片分割成$S\times S$网格，每一个网格单元负责去检测那些中心点落在该网格单元内的目标。每个网格单元会预测B个边界框以及边界框的置信度。置信度包含两个方面，一是这个网格单元含有目标的可能性的大小(Pr(object))，二是这个边界框的准确度，用预测框和实际框的IoU来表征($IOU_{pred}^{truth}$)，则置信度为$Pr(object)\times IOU_{pred}^{truth}$。边界框的大小和位置由x，y，w，h来表征（中心点预测的是每个中心点相对于对应网格单元左上角的偏移值）。再加上需要预测的类数C，则最终每一个网格单元需要预测$(B\times 5 + C)$个值，总共输出$S\times S \times (B\times 5 + C)$

**损失函数**：

YOLO将目标检测看成回归问题，因此采用均方误差损失函数，但是对不同的部分采用了不同的权重值。对于定位误差，即边界框的预测误差，采用较大的权重$\lambda_{coord}=5$。然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 $\lambda_{noobj}=0.5$ 。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为$(x, y, \sqrt{w}, \sqrt{h})$。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\yolov1.png)

另外一点是，由于每个单元格预测多个边界框。但是其对应类别只有一个。那么在训练时，如果该单元格内确实存在目标，那么只选择与ground truth的IOU最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。大家可能会想如果一个单元格内存在多个目标怎么办，其实这时候Yolo算法就只能选择其中一个来训练，这也是Yolo算法的缺点之一。要注意的一点时，对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。

**优点**：

1. 训练预测都是端到端的，算法简洁且速度快；
2. YOLO是对整张图片做卷积，所以其检测目标时有更大的感受野，相比于Faster R-CNN不容易对背景误判。

**缺点**：

1. YOLO各个单元格仅仅预测两个边界框，而且属于一个类别。对于小物体YOLO表现不好。
2. YOLO的定位不准确。

[参考资料：目标检测|YOLO原理与实现](https://zhuanlan.zhihu.com/p/32525231)

### YOLOv2改进

1. **Batch Normalization**: CNN在训练过程中网络每层输入的分布一直在改变, 会使训练过程难度加大，但可以通过normalize每层的输入解决这个问题,YOLOv2网络在每一个卷积层后添加batch normalization,同时舍弃了dropout，因为BN也能防止过拟合。
2. **高分辨率的分类器**：目标检测任务中，输入图像的分辨率越大，越有助于目标检测，YOLOv2将网络的输入提升到448\*448，它首先在分类网络上预训练一个448\*448输入的模型，然后基于这个模型进行fine tune。
3. **借鉴了Faster R-CNN中anchor的思想**：YOLO利用全连接层的数据完成边框的预测，导致丢失较多的空间信息，定位不准。YOLOv2引入anchor box来预测边界框。通过K-means聚类了5种不同的anchor作为每一个网格单元的预设anchor。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的boxes会比较小的boxes产生更多的error，聚类结果可能会偏离。为此，作者采用的评判标准是IOU得分（也就是boxes之间的交集除以并集），这样的话，error就和box的尺度无关了。
4. **直接位置预测**：使用anchor box时，发现模型不稳定，尤其是在早期迭代的时候。大部分的不稳定现象出现在预测box的坐标上。发现预测的时候采用Faster R-CNN的方式，可能会出现anchor去检测很远的目标的情况，效率比较低。正确做法应该是每一个anchor只负责检测周围正负一个单位以内的目标。模型随机初始化后，需要花很长一段时间才能稳定预测敏感的物体位置。
5. DarkNet-19:

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\loss.png)

[参考资料：YOLOv2算法详解](https://blog.csdn.net/wfei101/article/details/79398563)

### YOLOv3的改进

改进：

1. 基础网络采用DarkNet-53，YOLOv3中去掉了池化层和全连接层，特征图的尺寸变化是通过改变卷积核的步长来实现的。
2. YOLOv3借鉴了FPN的思想，采用多尺度来对不同size的目标进行检测，越精细的单元格就可以检测出越精细的物体。
3. 在预测anchor方面，YOLOv3沿用K-means聚类算法，聚类出9个anchor，在YOLOv3的每个层级依次分配三个，即每一层级的特征图上每个单元有三个anchor prior。
4. YOLOv3使用了logistic回归来对每个anchor包围的内容进行了一个目标性评分(objectness score)。
    根据目标性评分来选择anchor prior进行predict，而不是所有anchor prior都会有输出。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\yolov3.jpg)

[参考资料：yolo系列之yolov3](https://blog.csdn.net/leviopku/article/details/82660381)

## SSD相关问题

SSD属于单阶段的目标检测方法，SDD使用VGG16网络作为特征提取器，但是它将后面的全连接层替换成卷积层，并在之后添加了自定义的卷积层，然后直接在卷积层上进行检测，在多个特征图上设置不同尺度和不同长宽比的先验框进行检测，靠前的大尺度特征图可以捕获小物体的信息，用于检测小目标，而靠后的小尺度的特征图能捕获到大物体的信息，用于检测大目标。

### default boxes的设置

SSD中default boxes的概念有点像Faster R-CNN中的anchor，SSD在多个特征图上设置了default boxes，同样的每个位置设置了不同长宽比的default boxes。

SSD采用的难例负样本挖掘策略，即对负样本按照其预测背景类的置信度进行排序，选取置信度较小的top-k作为训练的负样本。

### SSD是如何预测的？

最后分别在所选的特征图上使用$3\times 3$的卷积核来预测不同default boxes所属的类别分数以及回归边界框的位置。然后将所有的检测结果通过nms操作，过滤掉冗余的检测框。

SSD的优势是速度比较快，整个过程只需要一步，但是对小尺寸的目标检测仍比较差。

## RetinaNet相关问题

### 什么是类别不平衡？类别不平衡会带来什么问题？

单阶段目标检测中负样本的数量极大于正样本的数量，比如包含目标的区域很少，不包含目标的区域很大，导致绝大多数的bbox属于背景。

由于大多数都是简单易分的负样本(属于背景的样本)，使得训练过程不能充分学习到属于那些有类别样本的信息，其次是简单易分的样本太多，可能掩盖了其他有类别样本的作用，也就是这些简单易分的样本主导了梯度更新的方向，掩盖了重要的信息，导致训练的分类器不佳。

### 为什么两阶段的检测器中不采用focal loss呢？

不是不能用，只是没太必要，因为两阶段的方法每个阶段都有对应的采样策略来缓解正负样本的不平衡了。

### ohem和Focal loss原理，Focal loss解决了什么问题，与ohem有什么区别？

one-stage精确度不如two-stage是因为下面的原因：

1. 正负样本比例极度不平衡。由于one-stage detector没有专门生成候选框的子网络，无法将候选框的数量减小到一个比较小的数量级（主流方法可以将候选框的数目减小到数千），导致了绝大多数候选框都是背景类，大大分散了放在非背景类上的精力；
2. 梯度被简单负样本主导。我们将背景类称为负样本。尽管单个负样本造成的loss很小，但是由于它们的数量极其巨大，对loss的总体贡献还是占优的，而真正应该主导loss的正样本由于数量较少，无法真正发挥作用。这样就导致收敛不到一个好的结果。

OHEM的核心思想就是增加错分类样本的权重，但是OHEM却忽略了易分类样本，而我们知道这一部分是所有样本中的绝大部分。与OHEM不同，Focal Loss把注意力放在了易分类样本上，它的形式如图所示。Focal Loss是一种可变比例的交叉熵损失，当正确分类可能性提高时比例系数会趋近于0。这样一来，即使再多的易分类样本也不会主导梯度下降的过程，于是训练网络自然可以自动对易分类样本降权，从而快速地集中处理难分类样本。

Focal loss ：

$$
FL(p_t) = -\alpha_t(1-p_t)^\gamma log(p_t)
$$

$\alpha$: 平衡参数，用于优化正负样本数据不平衡的情况。$\gamma$:调制系数：当$p_t$很小是，调节因子的值接近于1，loss不受影响，当$p_t$趋近于1时，调节因子接近于0，这样正确分类的样本点所占的loss大大降低，$\gamma$越大，易分类样本的重要性越低。

[参考资料](https://www.cnblogs.com/ranjiewen/p/10043151.html)

## 其他问题

### 比赛中类别不平衡怎么处理的？

1. 从数据的角度来考虑：可以通过数据增强来扩充少量样本类别；对多类样本进行欠采样，对少类样本进行过采样；人工合成数据。
2. 对数据代价加权，对少类样本数据赋予较大的权值，多类样本数据赋予较小的权值。
3. 损失函数的权值均衡，不同的类别loss的权重不一样，focal loss。

## 目标检测中IoU计算

### 水平框IoU计算

直接看代码！！！

#### 参考代码(Python 版本)

```Python
# x1[0] 表示第一个矩形的左上点的横坐标
# x1[1] 表示第二个矩形的左上点的横坐标
# 其他的类似
areas1 = (x2[0]-x1[0]+1)*(y2[0]-y1[0]+1)
areas2 = (x2[1]-x1[1]+1)*(y2[1]-y1[1]+1)

xx1 = np.maximum(x1[0], x1[1])
yy1 = np.maximum(y1[0], y1[1])
xx2 = np.minimum(x2[0], x2[1])
yy2 = np.minimum(y2[0], y2[1])

w = np.maximum(0.0, xx2 - xx1 + 1)
h = np.maximum(0.0, yy2 - yy1 + 1)
inter = w * h  # 计算交叠部分的面积
overlaps = inter / (areas1 + areas1 - inter) # 计算IoU
```

### 旋转框IoU计算

思路：首先计算两个矩形的所有交点，然后将交点集合按照顺时针排序，固定一个结点作为所有三角形的顶点，使用三角形剖分法计算IOU，即依次计算每一个三角形的面积，最后求和即可得到交叠区域的面积。

![](F:\WinProject\C++Algorithm\面试问题总结\imgs\RIoU.png)

## 非极大抑制(NMS)

用于过滤掉目标周围的冗余框，只保留最好的一个检测框。

### [NMS原理](https://wenliangsun.github.io/2019/06/10/nms/)

对于Bounding box的列表`B`和其对应的置信度`S`，采用下面的计算方式。

+ 选出具有最大score的检测框`M`，将其从`B`集合中移除并加入到最终检测结果`D`中。
+ 将`B`中剩余检测框中与`M`的`IoU`大于阈值`T`(一般取0.3~0.5)的框从`B`中移除。
+ 重复上述过程，直到`B`为空终止。

在上述过程中用到了排序，即对`B`中的bbox按照置信度`S`进行排序。

### 参考代码

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt


def nms(bboxes,scores,thresh):
    """
    bboxes -> [x1,y1,x2,y2] Nx4 输入的是左上和右下的坐标
    scores -> Nx1
    """
    x1 = bboxes[:,0]
    y1 = bboxes[:,1]
    x2 = bboxes[:,2]
    y2 = bboxes[:,3]
    order = scores.argsort()[::-1] # 对得分排序
    areas = (x2 - x1 + 1) * ( y2 - y1 + 1) # 计算bbox的面积
    keep = []

    while order.size > 0:
        i = order[0]
        keep.append(i)
        # 计算当前概率最大矩形框与其他矩形框的相交框的坐标
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2-xx1+1)
        h = np.maximum(0.0, yy2-yy1+1)
        inter = w * h  # 计算交叠部分的面积

        overlaps = inter/(areas[i] + areas[order[1:]] - inter) # 计算IoU

        inds = np.where(overlaps<=thresh)[0]
        order = order[inds+1]

    return keep


if __name__ == "__main__":
    img = cv2.imread("./demo.jpg")
    bboxes = [[105, 165, 290, 350],
              [150, 190, 340, 400],
              [80, 140, 250, 320],
              [153,130, 333, 330],
              [70, 199, 243, 406]]
    scores = [0.99, 0.6, 0.56, 0.7, 0.45]
    img2 = np.copy(img)
    bboxes = np.asarray(bboxes)
    scores = np.asarray(scores)

    keep = nms(bboxes, scores, 0.3)
    new_bboxes = bboxes[keep]
    new_scores = scores[keep]

    for i, box in enumerate(bboxes):
        x1, y1, x2, y2 = box
        cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2)
        cv2.putText(img,'%.2f'%scores[i], (x1,y1), 2, 1, (255, 255, 0))

    for i, box in enumerate(new_bboxes):
        x1, y1, x2, y2 = box
        cv2.rectangle(img2,(x1,y1),(x2,y2),(0,255,0),2)
        cv2.putText(img2,'%.2f'%new_scores[i], (x1,y1), 2, 1, (255, 255, 0))
    plt.figure()
    plt.imshow(img)
    plt.figure()
    plt.imshow(img2)
    plt.show()
```

上述代码只是针对单类目标进行NMS操作，如若想实现多类的NMS，只需要在外面套一层for 循环，每次对单一类别进行NMS，共进行类别数次。

### Soft-nms

上述NMS算法的一个主要问题是当两个ground truth的目标的确重叠度很高时,NMS会将具有较低置信度的框去掉(置信度改成0),**而soft NMS则是将和最大置信度重叠高于设定阈值的box的置信度降低。根据其IoU值，对重叠度高的置信度做一个基于连续函数的降值映射（decays the detection scores of all other objects as a continuous function of their overlap with Max）**。NMS直接对score置0，当前高IoU的bbox就直接丢弃，而soft NMS只会降低当前bbox的score，不会丢掉。

## 目标检测评价指标mAP

### mAP的定义和相关概念

+ mAP：mean Average Precision，即各类别AP的平均值
+ AP：PR曲线下的面积
+ PR曲线：Precision-Recall曲线
+ Precision：$\frac{TP}{TP+FP}$
+ Recall：$\frac{TP}{TP+FN}$
+ TP：$IoU>0.5$ 的检测框，但同一个Ground Truth 只计算一次
+ FP：$IoU<=0.5$ 的检测框，或者检测到同一个Ground Truth的多余检测框的数量
+ FN：没有检测到的Ground Truth的数量。

一般来说mAP是针对整个数据集而言的，AP正对数据集中的某一个类别而言的，而Precision和Recall是针对单张图片的某一类别的。

### 计算某一类别的AP

由上面的概念可知，当计算某一类别的AP时需要会画出这一类别的PR曲线，就需要先计算数据集中每张图片中这一类别的Precision和Recall。由公式：

+ $Precision=\frac{TP}{TP+FP}$
+ $Recall=\frac{TP}{TP+FN}$

我们只需要统计出相应的TP，FP和FN即可根据上述公式得出这一类别的Precision和Recall。

**如何判断TP，FP和FN？**

以单张图片为例，首先便利图片中的Ground Truth的目标，提取需要计算的某类别的Ground Truth 目标，之后读取通过检测器检测出来的这种类别的检测框（其他类别先不管），接着过滤掉置信度分数低于置信度阈值的框（也有的未设置信度阈值），将剩下的检测框按照置信度分数从高到低进行排序，最先判断置信度分数最高的检测框与gt box的iou是否大于设定的iou阈值，若大于设定的iou阈值，则将其判断为TP，并将该gt box 标记为已检测（后续的同一个gt的多余检测框都被标记为FP，这就是为什么先要按照置信度分数从高到低排序），若iou小于设定的阈值，则直接将其划分为FP。

```python
        if ovmax > ovthresh:              # 若iou大于阈值
            if not R['difficult'][jmax]:  # 且要检测的gt对象非difficult类型
                if not R['det'][jmax]:    # 且gt对象暂未被检测
                    tp[d] = 1.            # 此检测框被标记为TP
                    R['det'][jmax] = 1    # 将此gt目标标记为已检测
                else:
                    fp[d] = 1.            # 若gt对象已被检测，那么此检测框为FP
        else:
            fp[d] = 1.                    # iou小于阈值，此检测框为FP
```

FN的计算很简单，一幅图片中共有多少个gt是知道的，TP我们可以统计出来，用gt的数量减去TP的数量就可以获得FN的数量。至此，我们获得了TP，FP和FN，根据上面的公式即可计算出Precision和Recall。

**计算AP**

AP的计算三种方式：

+ 在VOC2010以前，只需要选取当recall>=0,0.1，0.2，...，1共11个点时的Precision最大的值，然后求这11个Precision的平均值就可得到AP值。
+ 在VOC2010及以后，需要针对每一个不同的Recall值（包括0和1），选取其大于等于这些recall值时的Precision的最大值，然后计算PR曲线下面的面积作为AP值。
+ COCO数据集，设定多个IOU阈值（0.5-0.95,0.05为步长），在每一个iou阈值下都有某一类别的AP值，然后求不同iou阈值下AP的均值，就是所求的最终的某一类别的AP值。

最后，mAP就是所有类的AP值的均值。以下是完整代码：

```python
def voc_ap(rec, prec, use_07_metric=False):
    """ ap = voc_ap(rec, prec, [use_07_metric])
    Compute VOC AP given precision and recall.
    If use_07_metric is true, uses the
    VOC 07 11 point method (default:False).
    """
    if use_07_metric:
        # 11 point metric
        ap = 0.
        for t in np.arange(0., 1.1, 0.1):
            if np.sum(rec >= t) == 0:
                p = 0
            else:
                p = np.max(prec[rec >= t])
            ap = ap + p / 11.
    else:
        # correct AP calculation
        # first append sentinel values at the end
        mrec = np.concatenate(([0.], rec, [1.]))
        mpre = np.concatenate(([0.], prec, [0.]))

        # compute the precision envelope
        for i in range(mpre.size - 1, 0, -1):
            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

        # to calculate area under PR curve, look for points
        # where X axis (recall) changes value
        i = np.where(mrec[1:] != mrec[:-1])[0]

        # and sum (\Delta recall) * prec
        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap

def voc_eval(detpath,
             annopath,
             imagesetfile,
             classname,
            # cachedir,
             ovthresh=0.5,
             use_07_metric=False):
    """rec, prec, ap = voc_eval(detpath,
                                annopath,
                                imagesetfile,
                                classname,
                                [ovthresh],
                                [use_07_metric])
    Top level function that does the PASCAL VOC evaluation.
    detpath: Path to detections
        detpath.format(classname) should produce the detection results file.
    annopath: Path to annotations
        annopath.format(imagename) should be the xml annotations file.
    imagesetfile: Text file containing the list of images, one image per line.
    classname: Category name (duh)
    cachedir: Directory for caching the annotations
    [ovthresh]: Overlap threshold (default = 0.5)
    [use_07_metric]: Whether to use VOC07's 11 point AP computation
        (default False)
    """
    # read list of images
    with open(imagesetfile, 'r') as f:
        lines = f.readlines()
    imagenames = [x.strip() for x in lines]
    recs = {}
    for i, imagename in enumerate(imagenames):
        recs[imagename] = parse_gt(annopath.format(imagename))
    # extract gt objects for this class
    class_recs = {}
    npos = 0
    for imagename in imagenames:
        R = [obj for obj in recs[imagename] if obj['name'] == classname]
        bbox = np.array([x['bbox'] for x in R])
        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)
        det = [False] * len(R)
        npos = npos + sum(~difficult)
        class_recs[imagename] = {'bbox': bbox,
                                 'difficult': difficult,
                                 'det': det}

    # read dets
    detfile = detpath.format(classname)
    with open(detfile, 'r') as f:
        lines = f.readlines()

    splitlines = [x.strip().split(' ') for x in lines]
    image_ids = [x[0] for x in splitlines]
    confidence = np.array([float(x[1]) for x in splitlines])

    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])
    # sort by confidence
    sorted_ind = np.argsort(-confidence)
    sorted_scores = np.sort(-confidence)
    BB = BB[sorted_ind, :]
    image_ids = [image_ids[x] for x in sorted_ind]
    # go down dets and mark TPs and FPs
    nd = len(image_ids)
    tp = np.zeros(nd)
    fp = np.zeros(nd)
    for d in range(nd):
        R = class_recs[image_ids[d]]
        bb = BB[d, :].astype(float)
        ovmax = -np.inf
        BBGT = R['bbox'].astype(float)
        if BBGT.size > 0:
            # compute overlaps
            ixmin = np.maximum(BBGT[:, 0], bb[0])
            iymin = np.maximum(BBGT[:, 1], bb[1])
            ixmax = np.minimum(BBGT[:, 2], bb[2])
            iymax = np.minimum(BBGT[:, 3], bb[3])
            iw = np.maximum(ixmax - ixmin + 1., 0.)
            ih = np.maximum(iymax - iymin + 1., 0.)
            inters = iw * ih
            # union
            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +
                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *
                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)
            overlaps = inters / uni
            ovmax = np.max(overlaps)
            jmax = np.argmax(overlaps)
        if ovmax > ovthresh:
            if not R['difficult'][jmax]:
                if not R['det'][jmax]:
                    tp[d] = 1.
                    R['det'][jmax] = 1
                else:
                    fp[d] = 1.
        else:
            fp[d] = 1.
    # compute precision recall
    print('npos num:', npos)
    fp = np.cumsum(fp)
    tp = np.cumsum(tp)
    print('check fp:', fp[-1])
    print('check tp', tp[-1])
    rec = tp / float(npos)
    # avoid divide by zero in case the first detection matches a difficult
    # ground truth
    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
    ap = voc_ap(rec, prec, use_07_metric)
    print('Recall: ', rec[-1])
    print('Prec: ', prec[-1])
    return rec, prec, ap
```
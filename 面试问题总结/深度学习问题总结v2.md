## 深度学习框架相关

### Pytorch和TensorFlow有什么区别？

Pytorch是动态框架，而TensorFlow是静态框架。对于TensorFlow，首先我们需要构造一个计算图，构建完之后，这个计算图就不能改变了，我们再开启会话，输入数据，进行计算，那么这个流程就是固定的很不灵活，因此它是静态框架。对于Pytorch，它的逻辑个Python是一样的，直接计算，不用会话，Pytorch的代码相对于TensorFlow代码更加简练。

### Pytorch中的交叉熵损失及其区别

1. BCELoss(二分类)：创建一个衡量目标和输出之间二进制交叉熵的评价指标。在输入这个损失函数之前需要将其通过sigmoid函数。
2. BCEWithLogitsLoss(二分类)：与BCELoss不同的是，它将Sigmoid函数和BCELoss方法结合到一个类中了，比BCELoss稳定，通过将操作合并到一层中，我们利用log-sum-exp技巧来实现数值稳定性。pos_weight(正样本的权重)
3. NLLLoss(多分类)：用于多分类的负对数似然损失函数(negative log likelihood loss)，在使用这个损失函数之前需要进行log_softmax操作。
4. CrossEntropyLoss(多分类)：它将nn.LogSoftmax()和nn.NLLLoss()方法结合到一个类中。当你有一个不平衡的训练集时，可选的参数weight权重应该是一个一维张量，为每个类分配权重。

[pytorch常用损失函数](https://www.cnblogs.com/wanghui-garcia/p/10862733.html)
[Pytorch中的分类问题损失函数](https://www.lizenghai.com/archives/61499.html)

### pytorch中的卷积层

```python
nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=2)
'''
in_channels=3:表示的是输入的通道数，由于是RGB型的，所以通道数是3.
out_channels=96:表示的是输出的通道数，设定输出通道数的96（这个是可以根据自己的需要来设置的）
kernel_size=12:表示卷积核的大小是12x12的，也就是上面的 “F”, F=12
stride=4:表示的是步长为4，也就是上面的S, S=4
padding=2:表示的是填充值的大小为2，也就是上面的P, P=2
'''
```

卷积神将网络的计算公式为：
N=(W-F+2P)/S+1
其中N：输出大小
W：输入大小
F：卷积核大小
P：填充值的大小
S：步长大小

### Pytorch中的转置卷积层

```python
class ConvTranspose2d(_ConvTransposeMixin, _ConvNd):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1,padding=0, output_padding=0, groups=1, bias=True,
                 dilation=1, padding_mode='zeros'):
    '''
    in_channels(int) – 输入信号的通道数
    out_channels(int) – 卷积产生的通道数
    kerner_size(int or tuple) - 卷积核的大小
    stride(int or tuple,optional) - 卷积步长
    padding(int or tuple, optional) - 输入的每一条边补充0的层数
    output_padding(int or tuple, optional) - 输出的每一条边补充0的层数
    dilation(int or tuple, optional) – 卷积核元素之间的间距
    groups(int, optional) – 从输入通道到输出通道的阻塞连接数
    bias(bool, optional) - 如果bias=True，添加偏置
    '''
```
步长大于1的反卷积在计算时，在其输入特征单元之间**插入strider-1个0**，插入0后把其看出是新的特征输入。**output_padding**参数在做步长为1的反卷积时是不用在意的。然而当步长大于1了，就需要手动设置以免网络出错。原因在于卷积操作时，比如输入$7\times 7$大小，步长为2，卷积核的尺寸为3，则输出$3\times 3$大小的特征图，这个卷积是完美的，但是如果输入$8\times 8$的大小，步长为2，卷积核的尺寸为3，则也输出$3\times 3$大小的特征图，当对$3\times 3$的输入进行步长为2的反卷积操作时，应该输出$7\times 7$还是$8\times 8$，这会导致有争议。

**由于卷积核滑动过程中，边界情况的不确定，使得在运算步长大于1的反卷积时会出现多种合法输出尺寸，pytorch的反卷积层提供了output_padding供使用者选择输出，一般情况下我们希望输入输出尺寸以步长为比例，因此output_padding一般取stride-1，同时padding取 (kernel_size - 1)/2 。**

计算公式为：

$$
H_{out} = (H_{in} - 1) \times S - 2 \times P + K + output\_padding
$$

[PyTorch中的转置卷积详解](https://blog.csdn.net/w55100/article/details/106467776/)
[pytorch反卷积层参数output_padding](https://blog.csdn.net/qq_41368247/article/details/86626446)
[ConvTranspose2d原理](https://blog.csdn.net/qq_27261889/article/details/86304061)

### Pytorch中卷积层和池化层的取整方式

Pytorch中的卷积操作或者是池化操作的H和W部分都是一样的计算公式，卷积操作在计算的时候如果不能整除的话是下取整，池化操作在计算的时候如果不能整除默认情况下也是下取整，但是它有一个
Pytorch中的卷积操作或者是池化操作的H和W部分都是一样的计算公式，卷积操作在计算的时候如果不能整除的话是下取整，池化操作在计算的时候如果不能整除默认情况下也是下取整，但是它有一个ceil_mode参数，当设置为True是，则会进行上取整。

$$
O_w = \lfloor \frac{I_w + 2 \times p - d \times (k - 1) - 1}{s} + 1 \rfloor \\
O_w = \lfloor \frac{I_w + 2 \times p - k}{s} + 1 \rfloor
$$

```python
class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):
"""
Parameters:	
    kernel_size – the size of the window to take a max over
    stride – the stride of the window. 默认值是kernel_size
    padding – implicit zero padding to be added on both side,默认值是0
    dilation – a parameter that controls the stride of elements in the window，默认值是1
    return_indices – if True, will return the max indices along with the outputs. Useful when Unpooling later
    ceil_mode – when True, will use ceil instead of floor to compute the output shape，向上取整和向下取整，默认是向下取整
"""

class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        pass
"""
Parameters:	
    in_channels (int) – Number of channels in the input image
    out_channels (int) – Number of channels produced by the convolution
    kernel_size (int or tuple) – Size of the convolving kernel
    stride (int or tuple, optional) – Stride of the convolution. Default: 1,默认是1
    padding (int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0
    dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1
    groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1
    bias (bool, optional) – If True, adds a learnable bias to the output. Default: True
"""
```

不一样的地方在于：第一点，步长stride默认值，上面默认和设定的kernel_size一样，下面默认是1；第二点，输出通道的不一样，上面的输出通道和输入通道是一样的，也就是没有改变特征图的数目，下面改变特征图的数目为out_channels；第三点不一样是卷积有一个参数groups,将特征图分开给不同的卷积进行操作然后再整合到一起。

[pytorch中的卷积和池化计算方式](https://blog.csdn.net/zz2230633069/article/details/83214308)

### Pytorch中卷积和池化过程中的padding是如何做的？

Pytorch的卷积层和池化层中的padding参数，可以是一个整数，也可以是一个元组，当是一个整数（如padding=1）时，它会在上下左右填充1行（1列）0值，使用的时候默认是从左到右使用，当只需要使用1行(1列)就足够的情况下，默认使用上面和左面的填充，而Keras(Tensorflow)中，如果只填充一行就足够了，则默认是在最后一行之后和最后一列之后填充。如果是一个元组，则元组的第一个参数表示在高度上面的padding，第二个参数表示在宽度上面的padding。

### Pytorch如何构建自己的模型(层)？

**torch.nn**：核心数据结构是Module,抽象的概念，既可以表示神经网络某个层layer，也可以表示一个包含很多层的神经网络。**常见做法是继承nn.Module,编写自己的层**。

1. 自定义层必须继承nn.Module，并且在其构造函数中需调用nn.Module的构造函数，super(xx,self).__init__()
2. 在构造函数__init__中必须自定义可学习的参数，并封装成Parameter
3. forward函数实现前向传播过程，其输入可以是一个或者多个tensor。无需写反向传播函数，nn.Module能够利用autograd自动实现反向传播，这比function简单的多
4. Module中可学习参数可以通过named_parameters()或者parameters()返回迭代器，前者会给每个parameter附上名字，使其更具有辨识度。

**torch.optim**： 将深度学习常用优化方法全部封装在torch.optim中，所有优化方法继承基类optim.Optimizer，并实现了自己的优化步骤。

1. optimizer = optim.SGD(param=net.parameters(),lr=1)
2. optimizer.zero_grad() #梯度清零，等价于net.zero_grad()
3. input = t.randn(1,3,32,32)
4. output = net(input)
5. output.backward(output)
6. optimizer.step()

**nn.functional**: nn中大多数layer，在function中都有一个与之相对应的函数。

nn.functional中的函数和nn.Module主要区别：

1. nn.Module实现的layers是一个特殊的类，都是有class layer(nn.Module)定义，会自动提取可学习的参数
2. nn.functional中的函数更像是纯函数，由def function(input)定义
3. 如果模型有可学习的参数，最好用nn.Module否则使用哪个都可以，二者在性能上没多大差异
4. 对于卷积，全连接等具有可学习参数的网络建议使用nn.Module
5. 激活函数（ReLU,sigmoid,tanh），池化等可以使用functional替代。对于不具有可学习参数的层，将他们用函数代替，这样可以不用放在构造函数__init__中。

**nn.init**：为初始化专门设计，包含各种初始化参数的方法。

案例：

```python
'''
pytorch 写模型的大致流程
'''

import torch.nn as nn
import matplotlib.pyplot as plt
import torchvision
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torchvision.transforms as transforms
import torch.functional.F as F


class Net(nn.Module):
    '''
    构建模型
    '''
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x  


if __name__ == "__main__":
    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]) # 预处理操作
    train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=trans) # 加载数据
    train_loader = DataLoader(train, batch_size=4, shuffle=True) # 构建迭代器
    model = Net() # 实例化网络
    criterion = nn.BCEWithLogitsLoss() # 定义损失函数
    optimizer = optim.Adam(model.parameters()) # 选择优化器
    model.train() # 设置模型为训练模式
    for e in range(1000):
        t_loss = 0
        for i, (data, label) in enumerate(train_loader): # 遍历数据集
            optimizer.zero_grad() # pytorch梯度会自动累加，需要手动清空上次计算的梯度
            outputs = model(data) # 前向传播
            loss = criterion(outputs, label) # 计算损失
            loss.backward() # 损失反向传递
            optimizer.step() # 优化器更新一步参数
            t_loss += loss.item() # 累计损失
```

[参考资料](https://blog.csdn.net/qq_21997625/article/details/105694092)

### Pytorch如何使用多GPU？

model.gpu() 把模型放在gpu上
model = nn.DataParallel(model) 。DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，各个GPU得到的梯度累加。与Module相关的所有数据也都会以浅复制的方式复制多份，在此需要注意，在module中属性应该是只读的。
对模型和相应的数据进行.cuda()处理，可以将内存中的数据复制到gpu显存中去。

```python
model = Model(input_size, output_size)
if torch.cuda.device_count() > 1:
  print("Let's use", torch.cuda.device_count(), "GPUs!")
  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs
  model = nn.DataParallel(model)

if torch.cuda.is_available():
   model.cuda()
```

### Pytorch多机多卡分布式训练

关于Pytorch分布训练的话，大家一开始接触的往往是DataParallel，这个wrapper能够很方便的使用多张卡，而且将进程控制在一个。唯一的问题就在于，DataParallel只能满足一台机器上gpu的通信，而一台机器一般只能装8张卡，对于一些大任务，8张卡就很吃力了，这个时候我们就需要面对多机多卡分布式训练这个问题了

1. **torch.nn.parallel.DistributedDataParallel**：与DataParallel相类似，也是一个模型wrapper。这个包是实现多机多卡分布训练最核心东西，它可以帮助我们在不同机器的多个模型拷贝之间平均梯度。
2. **torch.utils.data.distributed.DistributedSampler**：在多机多卡情况下分布式训练数据的读取也是一个问题，不同的卡读取到的数据应该是不同的。dataparallel的做法是直接将batch切分到不同的卡，这种方法对于多机来说不可取，因为多机之间直接进行数据传输会严重影响效率。于是有了利用sampler确保dataloader只会load到整个数据集的一个特定子集的做法。DistributedSampler就是做这件事的。它为每一个子进程划分出一部分数据集，以避免不同进程之间数据重复。

```python
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel

dataset = your_dataset()
datasampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
dataloader = DataLoader(dataset, batch_size=batch_size_per_gpu, sampler=datasampler)
model = your_model()
model = DistributedDataPrallel(model, device_ids=[local_rank], output_device=local_rank)
```

[Pytorch多机多卡分布式训练](https://zhuanlan.zhihu.com/p/68717029)

## 模型压缩方法有哪些？

模型压缩这一块的方法大致可以分为：低秩近似（low-rank Approximation），网络剪枝（network pruning），网络量化（network quantization），知识蒸馏（knowledge distillation）和紧凑网络设计（compact Network design）。

1. **低秩近似/分解**：如果把原先网络的权值矩阵当作满秩矩阵来看，那么可以用多个低秩的矩阵来逼近原来的矩阵，原先稠密的满秩矩阵可以表示为若干个低秩矩阵的组合，低秩矩阵又可以分解为小规模矩阵的乘积。早期的时候使用这种方法实现对网络的加速。
2. **网络剪枝**：网络剪枝的主要思想就是将权重矩阵中相对“不重要”的权值剔除，然后再重新fine tune 网络进行微调。
3. **网络量化**：一般而言，神经网络模型的参数都是用的32bit长度的浮点型数表示，实际上不需要保留那么高的精度，可以通过量化来降低每一个权值所需要占用的空间。根据量化方法不同，大致可以分为二值量化，三值量化，多值量化。
4. **知识蒸馏**：蒸馏模型采用的是迁移学习，通过采用预先训练好的复杂模型（Teacher model）的输出作为监督信号去训练另外一个简单的网络。
5. **紧凑网络设计**：在模型构建的初始阶段，就选择小而紧凑的网络，也就是紧凑网络设计。

[模型压缩总览](https://www.jianshu.com/p/e73851f32c9f)

## 如何解决神经网络训练时loss不下降的问题？

1. **模型结构和特征工程存在问题**：如果一个模型的结构有问题，那么它就很难训练，通过参考别人已经设计好并实现和测试过的结构，以及特征工程方案，进行改进和适应性修改，可以更快更好的完成目标任务。当模型结构不好或者规模太小、特征工程存在问题时，其对于数据的拟合能力不足，是很多人在进行一个新的研究或者工程应用时，遇到的第一个大问题。
2. **权重初始化方案有问题**：合适的初始化方案很重要，用对了，事半功倍，用不对，模型训练状况不忍直视。常用的初始化方案有全零初始化、随机正态分布初始化和随机均匀分布初始化等。
3. **正则化过度**：L1 L2和Dropout是防止过拟合用的，当训练集loss下不来时，就要考虑一下是不是正则化过度，导致模型欠拟合了。
4. **选择了不合适的激活函数、损失函数**：神经网络的激活函数、损失函数方面的选取，也是需要根据任务类型，选取最合适的。一般使用ReLu作为激活函数，因为可以有效避免梯度消失。
5. **训练时间不足**：多训练一会。
6. **数据未归一化**：未进行归一化会导致尺度的不平衡，使得梯度下降摇摆不定。
7. **batch size过大**：batch size过大时，模型前期由于梯度的平均，导致收敛速度过慢。

[如何解决神经网络训练时loss不下降的问题](https://blog.ailemon.me/2019/02/26/solution-to-loss-doesnt-drop-in-nn-train/)

## 模型参数初始化方法有哪些？

Pytorch在torch.nn.init中提供了常用的初始化方法函数。
[模型权值初始化的十种方法](https://www.jianshu.com/p/5e8e639fa08d)

### Xavier系列

是从"方差一致性"出发的，Xavier初始化分布有均匀分布和正态分布。

1. Xavier均匀分布：xavier初始化方法中服从均匀分布U(-a,a)，分布的参数a=gain*sqrt(6/fan_in+fan_out),这里有一个gain,增益的大小是依据激活函数类型来设定。`torch.nn.init.xavier_uniform_(tensor,gain=1)`
2. Xavier正态分布：xavier初始化方法中服从正态分布，
mean=0,std=gain*sqrt(2/fan_in+fan_out)。`torch.nn.init.xavier_normal_(tensor,gain=1)`。

### Kaiming系列

kaiming初始化方法，有何恺明大神提出来的初始化方法。公式推导同样从"方差一致性"出发，kaiming是针对xavier初始化方法在relu这一类激活函数表现不佳而提出的改进。

1. kaiming均匀分布：`torch.nn.init.kaiming_uniform(tensor,a=0,model='fan_in',nonlinearity='leaky_relu')`.此为均匀分布，U~(-bound,bound)，bound=sqrt(6/(1+a^2)*fan_in)其中，a为激活函数的负半轴的斜率，relu是0；mode- 可选为fan_in或fan_out,fan_in使用正向传播，方差一致；fan_out使反向传播时，方差一致nonlinearity-可选relu和leaky_relu，默认值为：leaky_relu。
2. kaiming正态分布：`torch.nn.init.kaiming_normal_(tensor,a=0,model='fan_in',nonlinearity='leaky_relu')`此为0均值的正态分布，N~(0,std),其中std=sqrt(2/(1+a^2)*fan_in)其中，a为激活函数的负半轴的斜率，relu是0；mode- 可选为fan_in或fan_out,fan_in使正向传播时，方差一致；fan_out使反向传播时，方差一致。nonlinearity-可选relu和leaky_relu，默认值为leaky_relu.

### 均匀分布初始化

`torch.nn.init.uniform_(tensor,a=0,b=1)` 使值服从均匀分布U(a,b)。

### 正态分布初始化

`torch.nn.init.normal_(tensor,mean=0,std=1)` 使值服从正态分布N(mean,std)，默认值为0，1。